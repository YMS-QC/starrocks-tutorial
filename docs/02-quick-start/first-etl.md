# ç¬¬ä¸€ä¸ªETLä»»åŠ¡

---

## ğŸ“– å¯¼èˆª

[ğŸ  è¿”å›ä¸»é¡µ](../../README.md) | [â¬…ï¸ ä¸Šä¸€é¡µ](connect-tools.md) | [â¡ï¸ ä¸‹ä¸€é¡µ](../03-table-design/table-models.md)

---

## å­¦ä¹ ç›®æ ‡

- æŒæ¡StarRocksæ•°æ®å¯¼å…¥çš„åŸºæœ¬æ–¹æ³•
- å­¦ä¼šä½¿ç”¨Stream Loadè¿›è¡Œæ‰¹é‡æ•°æ®å¯¼å…¥
- äº†è§£INSERTè¯­å¥çš„ä½¿ç”¨åœºæ™¯å’Œé™åˆ¶
- æŒæ¡åŸºç¡€çš„æ•°æ®è½¬æ¢å’Œæ¸…æ´—æŠ€å·§

## æ•°æ®å¯¼å…¥æ–¹å¼æ¦‚è§ˆ

| å¯¼å…¥æ–¹å¼ | é€‚ç”¨åœºæ™¯ | æ•°æ®é‡ | å®æ—¶æ€§ | å¤æ‚åº¦ |
|---------|---------|--------|--------|--------|
| **INSERT** | å°æ‰¹é‡ã€å®æ—¶å†™å…¥ | < 10ä¸‡è¡Œ | å®æ—¶ | ç®€å• |
| **Stream Load** | æ–‡ä»¶æ‰¹é‡å¯¼å…¥ | < 1000ä¸‡è¡Œ | å‡†å®æ—¶ | ä¸­ç­‰ |
| **Broker Load** | å¤§æ–‡ä»¶ã€HDFSå¯¼å…¥ | > 1000ä¸‡è¡Œ | ç¦»çº¿ | å¤æ‚ |
| **Routine Load** | Kafkaå®æ—¶æµ | è¿ç»­æµæ•°æ® | å®æ—¶ | ä¸­ç­‰ |

## å‡†å¤‡ç¤ºä¾‹æ•°æ®

### 1. åˆ›å»ºç¤ºä¾‹æ•°æ®åº“å’Œè¡¨

```sql
-- è¿æ¥StarRocks
mysql -h localhost -P 9030 -u root

-- åˆ›å»ºç¤ºä¾‹æ•°æ®åº“
CREATE DATABASE IF NOT EXISTS demo_etl;
USE demo_etl;

-- åˆ›å»ºç”¨æˆ·è¡¨ï¼ˆUniqueæ¨¡å‹ï¼Œæ”¯æŒæ›´æ–°ï¼‰
CREATE TABLE users (
    user_id BIGINT NOT NULL,
    username VARCHAR(50) NOT NULL,
    email VARCHAR(100),
    age INT,
    gender VARCHAR(10),
    city VARCHAR(50),
    register_date DATE,
    last_login DATETIME,
    status VARCHAR(20) DEFAULT 'ACTIVE',
    create_time DATETIME DEFAULT CURRENT_TIMESTAMP
)
UNIQUE KEY(user_id)
DISTRIBUTED BY HASH(user_id) BUCKETS 10
PROPERTIES (
    "replication_num" = "1",
    "enable_unique_key_merge_on_write" = "false"
);

-- åˆ›å»ºè®¢å•è¡¨ï¼ˆDuplicateæ¨¡å‹ï¼Œä¿ç•™æ˜ç»†ï¼‰
CREATE TABLE orders (
    order_id BIGINT NOT NULL,
    user_id BIGINT NOT NULL,
    product_id BIGINT NOT NULL,
    product_name VARCHAR(200),
    category VARCHAR(50),
    price DECIMAL(10,2),
    quantity INT,
    amount DECIMAL(10,2),
    order_date DATE,
    order_time DATETIME,
    status VARCHAR(20),
    create_time DATETIME DEFAULT CURRENT_TIMESTAMP
)
DUPLICATE KEY(order_id, user_id, order_time)
PARTITION BY RANGE(order_date) (
    PARTITION p202401 VALUES [('2024-01-01'), ('2024-02-01')),
    PARTITION p202402 VALUES [('2024-02-01'), ('2024-03-01')),
    PARTITION p202403 VALUES [('2024-03-01'), ('2024-04-01'))
)
DISTRIBUTED BY HASH(order_id) BUCKETS 10;

-- åˆ›å»ºé”€å”®æ±‡æ€»è¡¨ï¼ˆAggregateæ¨¡å‹ï¼Œé¢„èšåˆï¼‰
CREATE TABLE sales_summary (
    stat_date DATE NOT NULL,
    category VARCHAR(50) NOT NULL,
    total_amount DECIMAL(15,2) SUM DEFAULT "0",
    order_count BIGINT SUM DEFAULT "0",
    avg_price DECIMAL(10,2) REPLACE DEFAULT "0",
    max_price DECIMAL(10,2) MAX DEFAULT "0",
    min_price DECIMAL(10,2) MIN DEFAULT "999999",
    unique_users HLL HLL_UNION,
    update_time DATETIME REPLACE DEFAULT CURRENT_TIMESTAMP
)
AGGREGATE KEY(stat_date, category)
PARTITION BY RANGE(stat_date) (
    PARTITION p202401 VALUES [('2024-01-01'), ('2024-02-01')),
    PARTITION p202402 VALUES [('2024-02-01'), ('2024-03-01')),
    PARTITION p202403 VALUES [('2024-03-01'), ('2024-04-01'))
)
DISTRIBUTED BY HASH(category) BUCKETS 10;

-- æŸ¥çœ‹è¡¨ç»“æ„
SHOW CREATE TABLE users;
SHOW CREATE TABLE orders;
SHOW CREATE TABLE sales_summary;
```

### 2. ç”Ÿæˆç¤ºä¾‹æ•°æ®æ–‡ä»¶

```bash
# åˆ›å»ºç¤ºä¾‹æ•°æ®ç›®å½•
mkdir -p sample-data

# ç”Ÿæˆç”¨æˆ·æ•°æ®æ–‡ä»¶
cat > sample-data/users.csv << 'EOF'
user_id,username,email,age,gender,city,register_date,last_login,status
1001,alice,alice@example.com,25,F,åŒ—äº¬,2024-01-01,2024-01-15 10:30:00,ACTIVE
1002,bob,bob@example.com,30,M,ä¸Šæµ·,2024-01-02,2024-01-16 09:15:00,ACTIVE
1003,charlie,charlie@example.com,35,M,æ·±åœ³,2024-01-03,2024-01-17 14:20:00,ACTIVE
1004,diana,diana@example.com,28,F,å¹¿å·,2024-01-04,2024-01-18 16:45:00,ACTIVE
1005,eve,eve@example.com,32,F,æ­å·,2024-01-05,2024-01-19 11:10:00,INACTIVE
1006,frank,frank@example.com,27,M,å—äº¬,2024-01-06,2024-01-20 13:25:00,ACTIVE
1007,grace,grace@example.com,29,F,æˆéƒ½,2024-01-07,2024-01-21 15:35:00,ACTIVE
1008,henry,henry@example.com,31,M,æ­¦æ±‰,2024-01-08,2024-01-22 08:50:00,ACTIVE
1009,ivy,ivy@example.com,26,F,è¥¿å®‰,2024-01-09,2024-01-23 12:15:00,ACTIVE
1010,jack,jack@example.com,33,M,é•¿æ²™,2024-01-10,2024-01-24 17:40:00,ACTIVE
EOF

# ç”Ÿæˆè®¢å•æ•°æ®æ–‡ä»¶
cat > sample-data/orders.csv << 'EOF'
order_id,user_id,product_id,product_name,category,price,quantity,amount,order_date,order_time,status
2001,1001,3001,iPhone 15,æ‰‹æœº,7999.00,1,7999.00,2024-01-15,2024-01-15 10:30:00,PAID
2002,1002,3002,MacBook Pro,ç”µè„‘,16999.00,1,16999.00,2024-01-16,2024-01-16 09:15:00,PAID
2003,1003,3003,AirPods Pro,è€³æœº,1599.00,2,3198.00,2024-01-17,2024-01-17 14:20:00,PAID
2004,1001,3004,iPad Air,å¹³æ¿,4599.00,1,4599.00,2024-01-18,2024-01-18 11:45:00,PAID
2005,1004,3001,iPhone 15,æ‰‹æœº,7999.00,1,7999.00,2024-01-19,2024-01-19 16:45:00,PENDING
2006,1005,3005,Apple Watch,æ‰‹è¡¨,2999.00,1,2999.00,2024-01-20,2024-01-20 11:10:00,PAID
2007,1002,3003,AirPods Pro,è€³æœº,1599.00,1,1599.00,2024-01-21,2024-01-21 13:25:00,PAID
2008,1006,3002,MacBook Pro,ç”µè„‘,16999.00,1,16999.00,2024-01-22,2024-01-22 15:35:00,CANCELLED
2009,1007,3004,iPad Air,å¹³æ¿,4599.00,2,9198.00,2024-01-23,2024-01-23 08:50:00,PAID
2010,1003,3001,iPhone 15,æ‰‹æœº,7999.00,1,7999.00,2024-01-24,2024-01-24 12:15:00,PAID
EOF

# ç”Ÿæˆæ›´å¤šè®¢å•æ•°æ®ï¼ˆç”¨äºæ€§èƒ½æµ‹è¯•ï¼‰
python3 << 'EOF'
import csv
import random
from datetime import datetime, timedelta

# ç”Ÿæˆå¤§æ‰¹é‡è®¢å•æ•°æ®
products = [
    (3001, 'iPhone 15', 'æ‰‹æœº', 7999.00),
    (3002, 'MacBook Pro', 'ç”µè„‘', 16999.00),
    (3003, 'AirPods Pro', 'è€³æœº', 1599.00),
    (3004, 'iPad Air', 'å¹³æ¿', 4599.00),
    (3005, 'Apple Watch', 'æ‰‹è¡¨', 2999.00),
    (3006, 'iMac', 'ç”µè„‘', 12999.00),
    (3007, 'Apple TV', 'ç”µè§†', 1299.00),
    (3008, 'Magic Keyboard', 'é…ä»¶', 799.00),
    (3009, 'AirTag', 'é…ä»¶', 229.00),
    (3010, 'HomePod', 'éŸ³å“', 2299.00)
]

statuses = ['PAID', 'PENDING', 'CANCELLED', 'REFUNDED']
start_date = datetime(2024, 1, 1)

with open('sample-data/orders_bulk.csv', 'w', newline='', encoding='utf-8') as f:
    writer = csv.writer(f)
    writer.writerow(['order_id', 'user_id', 'product_id', 'product_name', 'category', 'price', 'quantity', 'amount', 'order_date', 'order_time', 'status'])
    
    for i in range(10000):  # ç”Ÿæˆ10000æ¡è®¢å•
        order_id = 10000 + i
        user_id = random.randint(1001, 1100)  # 100ä¸ªç”¨æˆ·
        product = random.choice(products)
        product_id, product_name, category, price = product
        quantity = random.randint(1, 3)
        amount = price * quantity
        
        # éšæœºæ—¥æœŸæ—¶é—´
        days_offset = random.randint(0, 89)  # 90å¤©å†…
        order_datetime = start_date + timedelta(days=days_offset, 
                                               hours=random.randint(0, 23), 
                                               minutes=random.randint(0, 59))
        order_date = order_datetime.date()
        status = random.choice(statuses)
        
        writer.writerow([order_id, user_id, product_id, product_name, category, 
                        price, quantity, amount, order_date, 
                        order_datetime.strftime('%Y-%m-%d %H:%M:%S'), status])

print("ç”Ÿæˆäº†10000æ¡è®¢å•æ•°æ®åˆ° sample-data/orders_bulk.csv")
EOF
```

## INSERTè¯­å¥å¯¼å…¥

### 1. å•è¡Œæ’å…¥

```sql
-- å•è¡Œæ’å…¥ç”¨æˆ·æ•°æ®
INSERT INTO users (user_id, username, email, age, gender, city, register_date) 
VALUES (2001, 'test_user', 'test@example.com', 25, 'M', 'åŒ—äº¬', '2024-01-01');

-- æŸ¥è¯¢éªŒè¯
SELECT * FROM users WHERE user_id = 2001;
```

### 2. æ‰¹é‡æ’å…¥

```sql
-- æ‰¹é‡æ’å…¥å¤šè¡Œæ•°æ®
INSERT INTO users (user_id, username, email, age, gender, city, register_date, status) VALUES
(2002, 'user2', 'user2@example.com', 28, 'F', 'ä¸Šæµ·', '2024-01-02', 'ACTIVE'),
(2003, 'user3', 'user3@example.com', 30, 'M', 'æ·±åœ³', '2024-01-03', 'ACTIVE'),
(2004, 'user4', 'user4@example.com', 32, 'F', 'å¹¿å·', '2024-01-04', 'ACTIVE');

-- æŸ¥è¯¢éªŒè¯
SELECT COUNT(*) FROM users WHERE user_id >= 2002;
```

### 3. INSERT INTO SELECT

```sql
-- ä»å…¶ä»–è¡¨æˆ–æŸ¥è¯¢ç»“æœæ’å…¥
-- åˆ›å»ºç”¨æˆ·ç»Ÿè®¡è¡¨
CREATE TABLE user_stats (
    city VARCHAR(50),
    user_count BIGINT,
    avg_age DECIMAL(5,2),
    stat_date DATE
) DISTRIBUTED BY HASH(city) BUCKETS 10;

-- ä½¿ç”¨INSERT INTO SELECTæ’å…¥ç»Ÿè®¡æ•°æ®
INSERT INTO user_stats (city, user_count, avg_age, stat_date)
SELECT 
    city,
    COUNT(*) as user_count,
    AVG(age) as avg_age,
    CURRENT_DATE as stat_date
FROM users 
WHERE status = 'ACTIVE'
GROUP BY city;

-- æŸ¥è¯¢éªŒè¯
SELECT * FROM user_stats;
```

## Stream Loadå¯¼å…¥

Stream Loadæ˜¯StarRocksæ¨èçš„æ‰¹é‡å¯¼å…¥æ–¹å¼ï¼Œé€‚åˆä¸­ç­‰è§„æ¨¡çš„æ•°æ®å¯¼å…¥ã€‚

### 1. åŸºç¡€Stream Load

```bash
# å¯¼å…¥ç”¨æˆ·æ•°æ®
curl --location-trusted -u root: \
    -H "label:load_users_$(date +%Y%m%d_%H%M%S)" \
    -H "format:csv" \
    -H "column_separator:," \
    -H "skip_header:1" \
    -T sample-data/users.csv \
    http://localhost:8030/api/demo_etl/users/_stream_load

# æ£€æŸ¥å¯¼å…¥ç»“æœ
mysql -h localhost -P 9030 -u root -e "SELECT COUNT(*) FROM demo_etl.users;"
```

### 2. å¸¦æ•°æ®è½¬æ¢çš„Stream Load

```bash
# å¯¼å…¥è®¢å•æ•°æ®ï¼Œå¸¦å­—æ®µæ˜ å°„å’Œè½¬æ¢
curl --location-trusted -u root: \
    -H "label:load_orders_$(date +%Y%m%d_%H%M%S)" \
    -H "format:csv" \
    -H "column_separator:," \
    -H "skip_header:1" \
    -H "columns:order_id,user_id,product_id,product_name,category,price,quantity,amount,order_date,order_time,status" \
    -H "where:status IN ('PAID', 'PENDING')" \
    -T sample-data/orders.csv \
    http://localhost:8030/api/demo_etl/orders/_stream_load

# éªŒè¯å¯¼å…¥ç»“æœ
mysql -h localhost -P 9030 -u root -e "
USE demo_etl;
SELECT status, COUNT(*) FROM orders GROUP BY status;
"
```

### 3. å¤§æ‰¹é‡æ•°æ®å¯¼å…¥

```bash
# å¯¼å…¥å¤§æ‰¹é‡è®¢å•æ•°æ®
curl --location-trusted -u root: \
    -H "label:load_orders_bulk_$(date +%Y%m%d_%H%M%S)" \
    -H "format:csv" \
    -H "column_separator:," \
    -H "skip_header:1" \
    -H "timeout:3600" \
    -H "max_filter_ratio:0.1" \
    -T sample-data/orders_bulk.csv \
    http://localhost:8030/api/demo_etl/orders/_stream_load

# æ£€æŸ¥å¯¼å…¥çŠ¶æ€
mysql -h localhost -P 9030 -u root -e "
USE demo_etl;
SELECT COUNT(*) as total_orders FROM orders;
SELECT status, COUNT(*) FROM orders GROUP BY status;
"
```

### 4. JSONæ•°æ®å¯¼å…¥

```bash
# åˆ›å»ºJSONæ ¼å¼çš„ç”¨æˆ·æ•°æ®
cat > sample-data/users.json << 'EOF'
{"user_id": 3001, "username": "json_user1", "email": "json1@example.com", "age": 25, "gender": "M", "city": "åŒ—äº¬"}
{"user_id": 3002, "username": "json_user2", "email": "json2@example.com", "age": 28, "gender": "F", "city": "ä¸Šæµ·"}
{"user_id": 3003, "username": "json_user3", "email": "json3@example.com", "age": 30, "gender": "M", "city": "æ·±åœ³"}
EOF

# å¯¼å…¥JSONæ•°æ®
curl --location-trusted -u root: \
    -H "label:load_users_json_$(date +%Y%m%d_%H%M%S)" \
    -H "format:json" \
    -H "strip_outer_array:false" \
    -H "jsonpaths:[\"$.user_id\", \"$.username\", \"$.email\", \"$.age\", \"$.gender\", \"$.city\"]" \
    -H "columns:user_id,username,email,age,gender,city,register_date,last_login,status,create_time" \
    -H "set:register_date=current_date(),last_login=now(),status='ACTIVE',create_time=now()" \
    -T sample-data/users.json \
    http://localhost:8030/api/demo_etl/users/_stream_load
```

## æ•°æ®è½¬æ¢å’Œæ¸…æ´—

### 1. æ•°æ®ç±»å‹è½¬æ¢

```sql
-- åˆ›å»ºåŸå§‹æ•°æ®è¡¨ï¼ˆæ‰€æœ‰å­—æ®µä¸ºå­—ç¬¦ä¸²ï¼‰
CREATE TABLE raw_data (
    user_id_str VARCHAR(50),
    age_str VARCHAR(10),
    amount_str VARCHAR(20),
    date_str VARCHAR(20)
) DISTRIBUTED BY HASH(user_id_str) BUCKETS 10;

-- æ’å…¥åŸå§‹æ•°æ®
INSERT INTO raw_data VALUES
('1001', '25', '1999.50', '2024-01-15'),
('1002', '30', '2500.00', '2024-01-16'),
('1003', 'unknown', '3200.75', '2024-01-17');

-- æ•°æ®è½¬æ¢æ’å…¥åˆ°ç›®æ ‡è¡¨
INSERT INTO users (user_id, username, age, register_date, status, create_time)
SELECT 
    CAST(user_id_str AS BIGINT) as user_id,
    CONCAT('user_', user_id_str) as username,
    CASE 
        WHEN age_str = 'unknown' THEN NULL
        ELSE CAST(age_str AS INT)
    END as age,
    CAST(date_str AS DATE) as register_date,
    'ACTIVE' as status,
    NOW() as create_time
FROM raw_data
WHERE user_id_str REGEXP '^[0-9]+$';  -- åªå¤„ç†æ•°å­—ID

-- æŸ¥è¯¢éªŒè¯
SELECT * FROM users WHERE user_id >= 1001 AND user_id <= 1003;
```

### 2. æ•°æ®æ¸…æ´—å’ŒéªŒè¯

```sql
-- åˆ›å»ºæ•°æ®è´¨é‡æ£€æŸ¥å‡½æ•°
-- æ£€æŸ¥é‚®ç®±æ ¼å¼
SELECT 
    user_id,
    email,
    CASE 
        WHEN email REGEXP '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$' 
        THEN 'VALID' 
        ELSE 'INVALID' 
    END as email_status
FROM users 
WHERE email IS NOT NULL;

-- æ£€æŸ¥å¹´é¾„åˆç†æ€§
SELECT 
    user_id,
    age,
    CASE 
        WHEN age IS NULL THEN 'NULL'
        WHEN age < 0 OR age > 120 THEN 'INVALID'
        ELSE 'VALID'
    END as age_status
FROM users;

-- ä¿®å¤æ•°æ®é—®é¢˜
UPDATE users 
SET age = NULL 
WHERE age < 0 OR age > 120;

-- åˆ é™¤æ— æ•ˆæ•°æ®
DELETE FROM users 
WHERE email IS NOT NULL 
  AND email NOT REGEXP '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$';
```

### 3. æ•°æ®èšåˆå’Œæ±‡æ€»

```sql
-- è®¡ç®—é”€å”®æ±‡æ€»æ•°æ®
INSERT INTO sales_summary (stat_date, category, total_amount, order_count, avg_price, max_price, min_price, unique_users, update_time)
SELECT 
    order_date as stat_date,
    category,
    SUM(amount) as total_amount,
    COUNT(*) as order_count,
    AVG(price) as avg_price,
    MAX(price) as max_price,
    MIN(price) as min_price,
    HLL_HASH(user_id) as unique_users,
    NOW() as update_time
FROM orders 
WHERE status = 'PAID'
  AND order_date >= '2024-01-01'
GROUP BY order_date, category;

-- æŸ¥è¯¢æ±‡æ€»ç»“æœ
SELECT 
    stat_date,
    category,
    total_amount,
    order_count,
    ROUND(avg_price, 2) as avg_price,
    HLL_CARDINALITY(unique_users) as unique_user_count
FROM sales_summary 
ORDER BY stat_date, category;
```

## å¯¼å…¥ç›‘æ§å’Œä¼˜åŒ–

### 1. æŸ¥çœ‹å¯¼å…¥å†å²

```sql
-- æŸ¥çœ‹Stream Loadå¯¼å…¥å†å²
SHOW LOAD\G

-- æŸ¥çœ‹å…·ä½“å¯¼å…¥ä»»åŠ¡è¯¦æƒ…
SHOW LOAD WHERE LABEL = 'your_label_name'\G

-- æŸ¥çœ‹æœ€è¿‘çš„å¯¼å…¥ä»»åŠ¡
SHOW LOAD ORDER BY CreateTime DESC LIMIT 5\G
```

### 2. å¯¼å…¥æ€§èƒ½ç›‘æ§

```sql
-- æŸ¥çœ‹è¡¨çš„æ•°æ®åˆ†å¸ƒ
SHOW DATA FROM orders;

-- æŸ¥çœ‹åˆ†åŒºæ•°æ®åˆ†å¸ƒ
SHOW PARTITIONS FROM orders\G

-- æŸ¥çœ‹å¯¼å…¥ä»»åŠ¡ç»Ÿè®¡
SELECT 
    `Database`,
    `Table`,
    COUNT(*) as load_count,
    SUM(LoadedRows) as total_rows,
    SUM(LoadBytes) as total_bytes
FROM information_schema.loads 
WHERE CreateTime >= DATE_SUB(NOW(), INTERVAL 1 DAY)
GROUP BY `Database`, `Table`;
```

### 3. æ€§èƒ½ä¼˜åŒ–å»ºè®®

```bash
# Stream Loadæ€§èƒ½ä¼˜åŒ–å‚æ•°
curl --location-trusted -u root: \
    -H "label:optimized_load_$(date +%Y%m%d_%H%M%S)" \
    -H "format:csv" \
    -H "column_separator:," \
    -H "skip_header:1" \
    -H "timeout:7200" \
    -H "max_filter_ratio:0.05" \
    -H "load_mem_limit:2147483648" \
    -H "exec_mem_limit:2147483648" \
    -H "strict_mode:false" \
    -H "partial_update:false" \
    -T sample-data/orders_bulk.csv \
    http://localhost:8030/api/demo_etl/orders/_stream_load
```

## é”™è¯¯å¤„ç†å’Œæ•…éšœæ’æŸ¥

### 1. å¸¸è§é”™è¯¯å¤„ç†

```sql
-- æŸ¥çœ‹å¯¼å…¥é”™è¯¯
SHOW LOAD WHERE State = 'CANCELLED' ORDER BY CreateTime DESC LIMIT 5\G

-- å¤„ç†æ•°æ®æ ¼å¼é”™è¯¯
-- åˆ›å»ºé”™è¯¯å¤„ç†è¡¨
CREATE TABLE error_log (
    error_time DATETIME,
    table_name VARCHAR(100),
    error_msg TEXT,
    data_sample TEXT
) DISTRIBUTED BY HASH(table_name) BUCKETS 10;
```

### 2. æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥

```sql
-- æ£€æŸ¥ä¸»é”®é‡å¤
SELECT user_id, COUNT(*) as cnt
FROM users 
GROUP BY user_id 
HAVING COUNT(*) > 1;

-- æ£€æŸ¥å¤–é”®çº¦æŸï¼ˆåº”ç”¨å±‚ï¼‰
SELECT o.user_id
FROM orders o
LEFT JOIN users u ON o.user_id = u.user_id
WHERE u.user_id IS NULL;

-- æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
SELECT 
    'users' as table_name,
    COUNT(*) as total_rows,
    COUNT(DISTINCT user_id) as unique_keys,
    COUNT(*) - COUNT(DISTINCT user_id) as duplicates
FROM users
UNION ALL
SELECT 
    'orders' as table_name,
    COUNT(*) as total_rows,
    COUNT(DISTINCT order_id) as unique_keys,
    COUNT(*) - COUNT(DISTINCT order_id) as duplicates
FROM orders;
```

## ETLè‡ªåŠ¨åŒ–è„šæœ¬

### 1. åˆ›å»ºETLè„šæœ¬

```bash
#!/bin/bash
# etl_pipeline.sh - ETLè‡ªåŠ¨åŒ–è„šæœ¬

set -e

LOG_FILE="logs/etl_$(date +%Y%m%d_%H%M%S).log"
mkdir -p logs

log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a $LOG_FILE
}

# 1. æ•°æ®å‡†å¤‡é˜¶æ®µ
log "å¼€å§‹ETLæµç¨‹..."

# 2. æ•°æ®å¯¼å…¥
log "å¯¼å…¥ç”¨æˆ·æ•°æ®..."
curl --location-trusted -u root: \
    -H "label:users_$(date +%Y%m%d_%H%M%S)" \
    -H "format:csv" \
    -H "column_separator:," \
    -H "skip_header:1" \
    -T sample-data/users.csv \
    http://localhost:8030/api/demo_etl/users/_stream_load

log "å¯¼å…¥è®¢å•æ•°æ®..."
curl --location-trusted -u root: \
    -H "label:orders_$(date +%Y%m%d_%H%M%S)" \
    -H "format:csv" \
    -H "column_separator:," \
    -H "skip_header:1" \
    -T sample-data/orders.csv \
    http://localhost:8030/api/demo_etl/orders/_stream_load

# 3. æ•°æ®éªŒè¯
log "éªŒè¯æ•°æ®..."
mysql -h localhost -P 9030 -u root -e "
USE demo_etl;
SELECT 'users' as table_name, COUNT(*) as row_count FROM users
UNION ALL
SELECT 'orders' as table_name, COUNT(*) as row_count FROM orders;
" >> $LOG_FILE

# 4. ç”Ÿæˆæ±‡æ€»æ•°æ®
log "ç”Ÿæˆæ±‡æ€»æ•°æ®..."
mysql -h localhost -P 9030 -u root -e "
USE demo_etl;
INSERT INTO sales_summary (stat_date, category, total_amount, order_count, avg_price, max_price, min_price, unique_users, update_time)
SELECT 
    order_date,
    category,
    SUM(amount),
    COUNT(*),
    AVG(price),
    MAX(price),
    MIN(price),
    HLL_HASH(user_id),
    NOW()
FROM orders 
WHERE status = 'PAID'
GROUP BY order_date, category
ON DUPLICATE KEY UPDATE
    total_amount = VALUES(total_amount),
    order_count = VALUES(order_count),
    avg_price = VALUES(avg_price),
    max_price = VALUES(max_price),
    min_price = VALUES(min_price),
    unique_users = VALUES(unique_users),
    update_time = VALUES(update_time);
"

log "ETLæµç¨‹å®Œæˆ"
```

### 2. åˆ›å»ºæ•°æ®ç›‘æ§è„šæœ¬

```bash
#!/bin/bash
# monitor_etl.sh - ETLç›‘æ§è„šæœ¬

# æ£€æŸ¥æ•°æ®é‡å˜åŒ–
mysql -h localhost -P 9030 -u root -e "
USE demo_etl;
SELECT 
    table_name,
    table_rows,
    data_length,
    update_time
FROM information_schema.tables 
WHERE table_schema = 'demo_etl'
  AND table_type = 'BASE TABLE';
"

# æ£€æŸ¥æœ€è¿‘çš„å¯¼å…¥ä»»åŠ¡
mysql -h localhost -P 9030 -u root -e "
SHOW LOAD ORDER BY CreateTime DESC LIMIT 10;
"
```

## æœ€ä½³å®è·µæ€»ç»“

### 1. æ•°æ®å¯¼å…¥é€‰æ‹©
- **å°æ‰¹é‡å®æ—¶**: ä½¿ç”¨INSERTè¯­å¥
- **ä¸­ç­‰æ‰¹é‡**: ä½¿ç”¨Stream Load
- **å¤§æ‰¹é‡ç¦»çº¿**: ä½¿ç”¨Broker Load
- **å®æ—¶æµæ•°æ®**: ä½¿ç”¨Routine Load

### 2. æ€§èƒ½ä¼˜åŒ–
- åˆç†è®¾ç½®åˆ†åŒºå’Œåˆ†æ¡¶
- æ§åˆ¶å¯¼å…¥æ‰¹æ¬¡å¤§å°
- å¹¶è¡Œå¯¼å…¥ä¸åŒåˆ†åŒº
- ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨

### 3. æ•°æ®è´¨é‡
- å¯¼å…¥å‰è¿›è¡Œæ•°æ®éªŒè¯
- è®¾ç½®åˆç†çš„é”™è¯¯å®¹å¿ç‡
- å»ºç«‹æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
- è®°å½•å’Œåˆ†æé”™è¯¯æ—¥å¿—

### 4. è¿ç»´ç®¡ç†
- è‡ªåŠ¨åŒ–ETLæµç¨‹
- ç›‘æ§å¯¼å…¥æ€§èƒ½
- å»ºç«‹å‘Šè­¦æœºåˆ¶
- å®šæœŸæ¸…ç†å†å²æ•°æ®

## å°ç»“

è¿™ä¸ªç¬¬ä¸€ä¸ªETLä»»åŠ¡æ¼”ç¤ºäº†ï¼š

1. **åŸºç¡€å¯¼å…¥**: INSERTå’ŒStream Loadçš„ä½¿ç”¨
2. **æ•°æ®è½¬æ¢**: ç±»å‹è½¬æ¢å’Œæ•°æ®æ¸…æ´—
3. **æ‰¹é‡å¤„ç†**: å¤§æ•°æ®é‡å¯¼å…¥æŠ€å·§
4. **ç›‘æ§è°ƒä¼˜**: æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–æ–¹æ³•
5. **è‡ªåŠ¨åŒ–**: ETLæµç¨‹è‡ªåŠ¨åŒ–è„šæœ¬

é€šè¿‡è¿™ä¸ªå®è·µï¼Œä½ å·²ç»æŒæ¡äº†StarRocksæ•°æ®å¯¼å…¥çš„åŸºæœ¬æŠ€èƒ½ï¼Œå¯ä»¥å¼€å§‹å¤„ç†çœŸå®çš„ä¸šåŠ¡æ•°æ®äº†ã€‚

---

## ğŸ“– å¯¼èˆª

[ğŸ  è¿”å›ä¸»é¡µ](../../README.md) | [â¬…ï¸ ä¸Šä¸€é¡µ](connect-tools.md) | [â¡ï¸ ä¸‹ä¸€é¡µ](../03-table-design/table-models.md)

---