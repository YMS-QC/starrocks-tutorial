---
## ğŸ“– å¯¼èˆª
[ğŸ  è¿”å›ä¸»é¡µ](../../README.md) | [â¬…ï¸ ä¸Šä¸€é¡µ](big-data-ecosystem.md) | [â¡ï¸ ä¸‹ä¸€é¡µ](../07-best-practices/oracle-migration-best-practices.md)
---

# StarRocksæ‰§è¡Œå¼•æ“æ·±åº¦è§£æ

> **ç‰ˆæœ¬è¦æ±‚**ï¼šæœ¬ç« èŠ‚æ·±å…¥å‰–æStarRocks 3.0+çš„æ‰§è¡Œå¼•æ“ï¼Œå»ºè®®ä½¿ç”¨3.1+ç‰ˆæœ¬ä»¥è·å¾—å®Œæ•´çš„Pipelineæ‰§è¡Œç‰¹æ€§

## å­¦ä¹ ç›®æ ‡

- é€šè¿‡ä¸€ä¸ªå¤æ‚SQLå®ä¾‹ï¼Œæ·±å…¥ç†è§£StarRocksçš„æŸ¥è¯¢æ‰§è¡Œå…¨æµç¨‹
- æŒæ¡Fragmentåˆ’åˆ†ã€å¹¶è¡Œæ‰§è¡Œã€æ•°æ®äº¤æ¢çš„æ ¸å¿ƒåŸç†
- ç†è§£Pipelineæ‰§è¡Œæ¨¡å‹å’Œå‘é‡åŒ–æ‰§è¡Œçš„æŠ€æœ¯ç»†èŠ‚
- äº†è§£æ‰§è¡Œä¼˜åŒ–æŠ€æœ¯å’Œæ€§èƒ½è°ƒä¼˜æ–¹æ³•

## ä¸€ã€å¤æ‚SQLç¤ºä¾‹è®¾è®¡

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªçœŸå®çš„ç”µå•†åˆ†æåœºæ™¯ï¼Œè®¾è®¡ä¸€ä¸ªæ¶µç›–å¤šç§æ‰§è¡Œç‰¹æ€§çš„å¤æ‚SQLï¼š

```sql
-- ä¸šåŠ¡åœºæ™¯ï¼šåˆ†æ2024å¹´1æœˆå„åŸå¸‚VIPç”¨æˆ·çš„è´­ä¹°è¡Œä¸ºå’Œå•†å“åå¥½
-- æ¶‰åŠç‰¹æ€§ï¼šå¤šè¡¨Joinã€èšåˆã€çª—å£å‡½æ•°ã€å­æŸ¥è¯¢ã€CTE

WITH user_city_stats AS (
    -- CTE1: è®¡ç®—å„åŸå¸‚VIPç”¨æˆ·æ•°é‡
    SELECT 
        city,
        COUNT(DISTINCT user_id) as vip_user_count,
        AVG(user_level) as avg_user_level
    FROM users
    WHERE is_vip = true 
      AND status = 'ACTIVE'
    GROUP BY city
),
order_metrics AS (
    -- CTE2: è®¡ç®—è®¢å•æŒ‡æ ‡
    SELECT 
        o.user_id,
        o.product_id,
        p.category,
        p.brand,
        o.order_date,
        o.amount,
        o.quantity,
        ROW_NUMBER() OVER (PARTITION BY o.user_id ORDER BY o.amount DESC) as order_rank
    FROM orders o
    INNER JOIN products p ON o.product_id = p.product_id
    WHERE o.order_date BETWEEN '2024-01-01' AND '2024-01-31'
      AND o.status = 'COMPLETED'
)
SELECT 
    u.city,
    ucs.vip_user_count,
    p.category as top_category,
    p.brand as top_brand,
    COUNT(DISTINCT om.user_id) as active_users,
    COUNT(om.order_id) as total_orders,
    SUM(om.amount) as total_revenue,
    AVG(om.amount) as avg_order_value,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY om.amount) as median_order_value,
    SUM(CASE WHEN om.order_rank = 1 THEN om.amount ELSE 0 END) as top_order_revenue
FROM order_metrics om
INNER JOIN users u ON om.user_id = u.user_id
INNER JOIN user_city_stats ucs ON u.city = ucs.city
LEFT JOIN product_reviews pr ON om.product_id = pr.product_id AND om.user_id = pr.user_id
WHERE u.is_vip = true
GROUP BY u.city, ucs.vip_user_count, p.category, p.brand
HAVING COUNT(DISTINCT om.user_id) >= 10
ORDER BY total_revenue DESC
LIMIT 100;
```

## äºŒã€æŸ¥è¯¢è§£æå’Œä¼˜åŒ–é˜¶æ®µ

### 2.1 SQLè§£æï¼ˆParseï¼‰

```
SQLæ–‡æœ¬
   â†“
[è¯æ³•åˆ†æå™¨ Lexer]
   â†“
Tokenæµï¼šWITH, user_city_stats, AS, (, SELECT, ...
   â†“
[è¯­æ³•åˆ†æå™¨ Parser]
   â†“
æŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTï¼‰
```

StarRocksä½¿ç”¨ANTLRç”Ÿæˆçš„Parserè§£æSQLï¼Œæ„å»ºASTï¼š

```
QueryStatement
â”œâ”€â”€ CTEClause
â”‚   â”œâ”€â”€ CTE: user_city_stats
â”‚   â”‚   â””â”€â”€ SelectStatement
â”‚   â””â”€â”€ CTE: order_metrics
â”‚       â””â”€â”€ SelectStatement
â””â”€â”€ SelectStatement (ä¸»æŸ¥è¯¢)
    â”œâ”€â”€ SelectList
    â”œâ”€â”€ FromClause
    â”œâ”€â”€ WhereClause
    â”œâ”€â”€ GroupByClause
    â”œâ”€â”€ HavingClause
    â”œâ”€â”€ OrderByClause
    â””â”€â”€ LimitClause
```

### 2.2 è¯­ä¹‰åˆ†æï¼ˆAnalyzeï¼‰

è¯­ä¹‰åˆ†æé˜¶æ®µéªŒè¯SQLçš„æ­£ç¡®æ€§å¹¶ç»‘å®šå…ƒæ•°æ®ï¼š

```java
// ä¼ªä»£ç å±•ç¤ºè¯­ä¹‰åˆ†æè¿‡ç¨‹
class Analyzer {
    void analyze(QueryStatement stmt) {
        // 1. è§£æCTE
        analyzeCTEs(stmt.getCTEs());
        
        // 2. è§£æFROMå­å¥ï¼Œæ„å»ºè¡¨å¼•ç”¨
        analyzeFrom(stmt.getFromClause());
        
        // 3. è§£æJOINæ¡ä»¶
        analyzeJoins(stmt.getJoins());
        
        // 4. è§£æWHEREæ¡ä»¶
        analyzeWhere(stmt.getWhereClause());
        
        // 5. è§£æGROUP BY
        analyzeGroupBy(stmt.getGroupByClause());
        
        // 6. è§£æèšåˆå‡½æ•°
        analyzeAggregates(stmt.getSelectList());
        
        // 7. è§£æçª—å£å‡½æ•°
        analyzeWindowFunctions(stmt.getWindowFunctions());
        
        // 8. ç±»å‹æ¨å¯¼å’Œæ£€æŸ¥
        inferAndCheckTypes(stmt);
    }
}
```

### 2.3 æŸ¥è¯¢ä¼˜åŒ–ï¼ˆOptimizeï¼‰

#### 2.3.1 é€»è¾‘ä¼˜åŒ–

StarRocksçš„é€»è¾‘ä¼˜åŒ–å™¨åº”ç”¨å„ç§è§„åˆ™ï¼ˆRuleï¼‰ä¼˜åŒ–æŸ¥è¯¢ï¼š

```
åŸå§‹é€»è¾‘è®¡åˆ’
    â†“
[è§„åˆ™ä¼˜åŒ–å™¨ RBO]
â”œâ”€â”€ è°“è¯ä¸‹æ¨ï¼ˆPredicate Pushdownï¼‰
â”œâ”€â”€ åˆ—è£å‰ªï¼ˆColumn Pruningï¼‰
â”œâ”€â”€ å¸¸é‡æŠ˜å ï¼ˆConstant Foldingï¼‰
â”œâ”€â”€ å­æŸ¥è¯¢å±•å¼€ï¼ˆSubquery Unnestingï¼‰
â”œâ”€â”€ å¤–è¿æ¥æ¶ˆé™¤ï¼ˆOuter Join Eliminationï¼‰
â””â”€â”€ å…¬å…±è¡¨è¾¾å¼æ¶ˆé™¤ï¼ˆCommon Expression Eliminationï¼‰
    â†“
ä¼˜åŒ–åçš„é€»è¾‘è®¡åˆ’
```

å…·ä½“ä¼˜åŒ–ç¤ºä¾‹ï¼š

```sql
-- è°“è¯ä¸‹æ¨å‰
Filter (u.is_vip = true)
  â””â”€â”€ Join (om.user_id = u.user_id)
      â”œâ”€â”€ Scan orders
      â””â”€â”€ Scan users

-- è°“è¯ä¸‹æ¨å
Join (om.user_id = u.user_id)
  â”œâ”€â”€ Scan orders
  â””â”€â”€ Filter (is_vip = true)
      â””â”€â”€ Scan users  -- è¿‡æ»¤æ¡ä»¶ä¸‹æ¨åˆ°Scan
```

#### 2.3.2 ç‰©ç†ä¼˜åŒ–ï¼ˆCBOï¼‰

åŸºäºæˆæœ¬çš„ä¼˜åŒ–å™¨é€‰æ‹©æœ€ä¼˜çš„ç‰©ç†æ‰§è¡Œè®¡åˆ’ï¼š

```java
// CBOæ ¸å¿ƒæµç¨‹
class CostBasedOptimizer {
    PhysicalPlan optimize(LogicalPlan logical) {
        // 1. æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        Statistics stats = collectStatistics(logical);
        
        // 2. æšä¸¾å¯èƒ½çš„Joiné¡ºåº
        List<JoinOrder> joinOrders = enumerateJoinOrders(logical);
        
        // 3. é€‰æ‹©Joinç®—æ³•
        for (JoinOrder order : joinOrders) {
            // è¯„ä¼°Broadcast Joinæˆæœ¬
            double broadcastCost = estimateBroadcastCost(order, stats);
            
            // è¯„ä¼°Shuffle Joinæˆæœ¬
            double shuffleCost = estimateShuffleCost(order, stats);
            
            // è¯„ä¼°Colocation Joinæˆæœ¬
            double colocationCost = estimateColocationCost(order, stats);
            
            // é€‰æ‹©æˆæœ¬æœ€ä½çš„æ–¹æ¡ˆ
            selectBestJoinMethod(order, broadcastCost, shuffleCost, colocationCost);
        }
        
        // 4. ç”Ÿæˆæœ€ä¼˜ç‰©ç†è®¡åˆ’
        return generatePhysicalPlan(bestJoinOrder);
    }
}
```

## ä¸‰ã€ç‰©ç†æ‰§è¡Œè®¡åˆ’ç”Ÿæˆ

### 3.1 Fragmentåˆ’åˆ†

StarRockså°†ç‰©ç†è®¡åˆ’åˆ’åˆ†ä¸ºå¤šä¸ªFragmentï¼Œæ¯ä¸ªFragmentå¯ä»¥ç‹¬ç«‹å¹¶è¡Œæ‰§è¡Œï¼š

```
å®Œæ•´ç‰©ç†è®¡åˆ’
    â†“
Fragmentåˆ’åˆ†è§„åˆ™ï¼š
1. é‡åˆ°Exchangeç®—å­æ—¶åˆ’åˆ†æ–°Fragment
2. æ¯ä¸ªFragmentå†…éƒ¨çš„ç®—å­åœ¨åŒä¸€ä¸ªPipelineä¸­æ‰§è¡Œ
3. Fragmentä¹‹é—´é€šè¿‡ç½‘ç»œäº¤æ¢æ•°æ®
    â†“
å¤šä¸ªFragment
```

æˆ‘ä»¬çš„å¤æ‚SQLä¼šè¢«åˆ’åˆ†ä¸ºä»¥ä¸‹Fragmentï¼š

```
Fragment 0 (Coordinator Fragment)
â”œâ”€â”€ Limit 100
â”œâ”€â”€ Sort (total_revenue DESC)
â””â”€â”€ Exchange (GATHER)

Fragment 1 (Aggregation Fragment)
â”œâ”€â”€ Aggregate (GROUP BY city, category, brand)
â”œâ”€â”€ Having (active_users >= 10)
â”œâ”€â”€ Hash Join (om.user_id = u.user_id)
â”‚   â”œâ”€â”€ Exchange (SHUFFLE by user_id)
â”‚   â””â”€â”€ Exchange (SHUFFLE by user_id)

Fragment 2 (Order Metrics Fragment)
â”œâ”€â”€ Project
â”œâ”€â”€ Window Function (ROW_NUMBER)
â”œâ”€â”€ Hash Join (o.product_id = p.product_id)
â”‚   â”œâ”€â”€ Exchange (BROADCAST)
â”‚   â””â”€â”€ Scan orders (with predicates)

Fragment 3 (User Stats Fragment)
â”œâ”€â”€ Aggregate (GROUP BY city)
â”œâ”€â”€ Filter (is_vip = true AND status = 'ACTIVE')
â””â”€â”€ Scan users

Fragment 4 (Product Fragment)
â”œâ”€â”€ Project
â””â”€â”€ Scan products

Fragment 5 (Review Fragment)
â”œâ”€â”€ Project
â””â”€â”€ Scan product_reviews
```

### 3.2 Fragmentå®ä¾‹åŒ–

æ¯ä¸ªFragmentä¼šåœ¨å¤šä¸ªBEèŠ‚ç‚¹ä¸Šåˆ›å»ºå¤šä¸ªå®ä¾‹å¹¶è¡Œæ‰§è¡Œï¼š

```java
// Fragmentå®ä¾‹åŒ–è¿‡ç¨‹
class FragmentExecutor {
    void executeFragment(Fragment fragment, int parallelism) {
        // 1. è®¡ç®—å¹¶è¡Œåº¦
        int dop = calculateDOP(fragment, parallelism);
        
        // 2. åˆ†é…æ‰§è¡ŒèŠ‚ç‚¹
        List<Backend> backends = selectBackends(fragment, dop);
        
        // 3. åˆ›å»ºFragmentå®ä¾‹
        for (int i = 0; i < dop; i++) {
            Backend backend = backends.get(i % backends.size());
            FragmentInstance instance = new FragmentInstance(
                fragment, 
                i,           // instance_id
                backend,     // æ‰§è¡ŒèŠ‚ç‚¹
                dop          // æ€»å¹¶è¡Œåº¦
            );
            
            // 4. å‘é€åˆ°BEæ‰§è¡Œ
            backend.executeInstance(instance);
        }
    }
    
    int calculateDOP(Fragment fragment, int requestedParallelism) {
        // åŸºäºæ•°æ®é‡ã€è¡¨çš„Tabletæ•°é‡ã€ç³»ç»Ÿèµ„æºç­‰è®¡ç®—
        int dataSize = fragment.estimateDataSize();
        int tabletCount = fragment.getTabletCount();
        int availableCores = getAvailableCores();
        
        return Math.min(
            requestedParallelism,
            Math.min(tabletCount, availableCores)
        );
    }
}
```

## å››ã€Pipelineæ‰§è¡Œæ¨¡å‹

### 4.1 Pipelineæ¦‚å¿µ

StarRocks 2.5+å¼•å…¥äº†Pipelineæ‰§è¡Œæ¨¡å‹ï¼Œå°†ç®—å­ç»„ç»‡æˆPipelineï¼š

```
ä¼ ç»Ÿç«å±±æ¨¡å‹ vs Pipelineæ¨¡å‹

ç«å±±æ¨¡å‹ï¼ˆæ‹‰å–æ¨¡å¼ï¼‰ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Limit  â”‚ â† next()
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
â”‚  Sort   â”‚ â† next()
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
â”‚Aggregateâ”‚ â† next()
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
â”‚  Join   â”‚ â† next()
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜

Pipelineæ¨¡å‹ï¼ˆæ¨é€æ¨¡å¼ï¼‰ï¼š
Pipeline 1: Scan â†’ Filter â†’ Build HashTable
Pipeline 2: Scan â†’ Probe HashTable â†’ Aggregate â†’ Sort â†’ Limit
```

### 4.2 Pipelineæ„å»º

```java
// Pipelineæ„å»ºç®—æ³•
class PipelineBuilder {
    List<Pipeline> buildPipelines(Fragment fragment) {
        List<Pipeline> pipelines = new ArrayList<>();
        
        // 1. è¯†åˆ«Pipeline breakerï¼ˆé˜»å¡ç®—å­ï¼‰
        List<Operator> breakers = findPipelineBreakers(fragment);
        // Breakers: HashJoinBuild, Sort, Aggregate(éœ€è¦å…¨é‡æ•°æ®)
        
        // 2. æ ¹æ®breakeråˆ’åˆ†Pipeline
        Pipeline currentPipeline = new Pipeline();
        for (Operator op : fragment.getOperators()) {
            if (breakers.contains(op)) {
                // é‡åˆ°breakerï¼Œç»“æŸå½“å‰Pipeline
                pipelines.add(currentPipeline);
                currentPipeline = new Pipeline();
            }
            currentPipeline.addOperator(op);
        }
        pipelines.add(currentPipeline);
        
        // 3. è®¾ç½®Pipelineä¾èµ–å…³ç³»
        setPipelineDependencies(pipelines);
        
        return pipelines;
    }
}
```

### 4.3 Pipelineæ‰§è¡Œ

```java
// Pipelineæ‰§è¡Œå™¨
class PipelineExecutor {
    void executePipeline(Pipeline pipeline) {
        // 1. åˆå§‹åŒ–Pipelineé©±åŠ¨å™¨
        List<PipelineDriver> drivers = createDrivers(pipeline);
        
        // 2. å¹¶å‘æ‰§è¡Œdrivers
        ThreadPool executor = getThreadPool();
        for (PipelineDriver driver : drivers) {
            executor.submit(() -> {
                while (!driver.isFinished()) {
                    // 3. æ‰§è¡Œä¸€ä¸ªchunkçš„æ•°æ®
                    Chunk chunk = driver.pullChunk();
                    
                    // 4. é€ä¸ªç®—å­å¤„ç†
                    for (Operator op : pipeline.getOperators()) {
                        chunk = op.process(chunk);
                        if (chunk == null) break; // è¢«è¿‡æ»¤æ‰
                    }
                    
                    // 5. è¾“å‡ºåˆ°ä¸‹ä¸€ä¸ªPipelineæˆ–ç»“æœé›†
                    if (chunk != null) {
                        driver.pushChunk(chunk);
                    }
                }
            });
        }
    }
}
```

## äº”ã€å‘é‡åŒ–æ‰§è¡Œ

### 5.1 å‘é‡åŒ–åŸç†

StarRocksé‡‡ç”¨åˆ—å¼å­˜å‚¨å’Œå‘é‡åŒ–æ‰§è¡Œï¼Œä¸€æ¬¡å¤„ç†ä¸€æ‰¹æ•°æ®ï¼ˆChunkï¼‰ï¼š

```cpp
// ä¼ ç»Ÿè¡Œå¼æ‰§è¡Œ vs å‘é‡åŒ–æ‰§è¡Œ

// è¡Œå¼æ‰§è¡Œï¼ˆé€è¡Œå¤„ç†ï¼‰
for (int i = 0; i < rows.size(); i++) {
    Row row = rows[i];
    if (row.age > 25) {  // åˆ†æ”¯é¢„æµ‹å¼€é”€
        result.add(row);
    }
}

// å‘é‡åŒ–æ‰§è¡Œï¼ˆæ‰¹é‡å¤„ç†ï¼‰
class VectorizedFilter {
    Chunk process(Chunk input) {
        // 1. è·å–åˆ—å‘é‡
        ColumnVector ageColumn = input.getColumn("age");
        
        // 2. SIMDæ‰¹é‡æ¯”è¾ƒ
        BitmapVector selection = SIMD::greater_than(
            ageColumn.getData(),  // int32_tæ•°ç»„
            25,                   // å¸¸é‡
            ageColumn.size()      // å‘é‡é•¿åº¦
        );
        
        // 3. æ ¹æ®selectionè¿‡æ»¤
        return input.filter(selection);
    }
}
```

### 5.2 SIMDä¼˜åŒ–

åˆ©ç”¨CPUçš„SIMDæŒ‡ä»¤é›†åŠ é€Ÿè®¡ç®—ï¼š

```cpp
// AVX2æŒ‡ä»¤é›†ç¤ºä¾‹ï¼šæ‰¹é‡è®¡ç®— amount * quantity
void vectorizedMultiply(
    const double* amount,    // é‡‘é¢æ•°ç»„
    const int32_t* quantity, // æ•°é‡æ•°ç»„
    double* result,          // ç»“æœæ•°ç»„
    size_t size
) {
    size_t simd_size = size - (size % 4);  // AVX2ä¸€æ¬¡å¤„ç†4ä¸ªdouble
    
    for (size_t i = 0; i < simd_size; i += 4) {
        // åŠ è½½æ•°æ®åˆ°256ä½å¯„å­˜å™¨
        __m256d amt = _mm256_loadu_pd(&amount[i]);
        __m256i qty = _mm256_loadu_si256((__m256i*)&quantity[i]);
        
        // è½¬æ¢intåˆ°double
        __m256d qty_d = _mm256_cvtepi32_pd(qty);
        
        // SIMDä¹˜æ³•
        __m256d res = _mm256_mul_pd(amt, qty_d);
        
        // å­˜å‚¨ç»“æœ
        _mm256_storeu_pd(&result[i], res);
    }
    
    // å¤„ç†å‰©ä½™å…ƒç´ 
    for (size_t i = simd_size; i < size; i++) {
        result[i] = amount[i] * quantity[i];
    }
}
```

### 5.3 å‘é‡åŒ–èšåˆ

```cpp
// å‘é‡åŒ–GROUP BYèšåˆ
class VectorizedAggregator {
    void aggregate(Chunk input, AggregationState* state) {
        // 1. è®¡ç®—åˆ†ç»„é”®çš„Hash
        std::vector<uint32_t> hashes(input.numRows());
        for (auto& key_column : group_by_columns) {
            key_column->updateHash(hashes);
        }
        
        // 2. æŸ¥æ‰¾æˆ–åˆ›å»ºèšåˆçŠ¶æ€
        for (size_t i = 0; i < input.numRows(); i++) {
            AggState* agg = state->findOrCreate(hashes[i]);
            
            // 3. æ‰¹é‡æ›´æ–°èšåˆå‡½æ•°
            for (auto& agg_func : agg_functions) {
                agg_func->addBatch(agg, input, i);
            }
        }
    }
    
    // COUNT(DISTINCT)çš„å‘é‡åŒ–å®ç°
    void countDistinct(ColumnVector values) {
        // ä½¿ç”¨HyperLogLogè¿›è¡Œè¿‘ä¼¼è®¡ç®—
        HyperLogLog hll;
        
        // æ‰¹é‡æ·»åŠ 
        for (size_t i = 0; i < values.size(); i += BATCH_SIZE) {
            size_t batch_end = std::min(i + BATCH_SIZE, values.size());
            hll.addBatch(values.getData() + i, batch_end - i);
        }
        
        return hll.estimate();
    }
}
```

## å…­ã€æ•°æ®äº¤æ¢ï¼ˆExchangeï¼‰

### 6.1 Exchangeç±»å‹

```cpp
enum ExchangeType {
    GATHER,      // æ±‡é›†åˆ°Coordinator
    BROADCAST,   // å¹¿æ’­åˆ°æ‰€æœ‰èŠ‚ç‚¹
    HASH_SHUFFLE,// æŒ‰Hashåˆ†åŒº
    RANDOM,      // éšæœºåˆ†å‘
    BUCKET       // æŒ‰æ¡¶åˆ†å‘
};

class ExchangeNode {
    void execute() {
        switch (type) {
            case BROADCAST:
                broadcastData();
                break;
            case HASH_SHUFFLE:
                hashPartition();
                break;
            case GATHER:
                gatherToCoordinator();
                break;
        }
    }
    
    void hashPartition() {
        // 1. å¯¹æ¯è¡Œæ•°æ®è®¡ç®—Hash
        for (Chunk chunk : input_chunks) {
            for (int row = 0; row < chunk.numRows(); row++) {
                uint32_t hash = computeHash(chunk, row, partition_exprs);
                int target_instance = hash % num_instances;
                
                // 2. å‘é€åˆ°ç›®æ ‡å®ä¾‹
                sendRow(chunk, row, target_instance);
            }
        }
    }
}
```

### 6.2 ç½‘ç»œä¼ è¾“ä¼˜åŒ–

```cpp
// æ‰¹é‡å‘é€å’Œå‹ç¼©
class DataStreamSender {
private:
    // æ¯ä¸ªç›®æ ‡å®ä¾‹çš„ç¼“å†²åŒº
    std::vector<ChunkBuffer> buffers;
    
public:
    void sendChunk(Chunk chunk) {
        // 1. åºåˆ—åŒ–Chunk
        std::string serialized = serialize(chunk);
        
        // 2. å‹ç¼©ï¼ˆLZ4/Snappyï¼‰
        std::string compressed = compress(serialized);
        
        // 3. æ‰¹é‡å‘é€
        for (int i = 0; i < num_receivers; i++) {
            buffers[i].add(compressed);
            
            if (buffers[i].size() >= BATCH_SIZE) {
                // è¾¾åˆ°æ‰¹é‡å¤§å°ï¼Œå‘é€
                sendBatch(i, buffers[i]);
                buffers[i].clear();
            }
        }
    }
    
    // RPCå‘é€
    void sendBatch(int receiver_id, ChunkBuffer& buffer) {
        TransmitDataRequest request;
        request.set_compressed_data(buffer.getData());
        request.set_compression_type(LZ4);
        
        // å¼‚æ­¥RPC
        stub->transmitData(request, [](Response resp) {
            // å¤„ç†å“åº”
        });
    }
}
```

## ä¸ƒã€Runtime Filter

### 7.1 Runtime FilteråŸç†

Runtime Filteråœ¨Joinæ‰§è¡Œæ—¶åŠ¨æ€ç”Ÿæˆè¿‡æ»¤å™¨ï¼Œæå‰è¿‡æ»¤æ•°æ®ï¼š

```
Hash Joinæ‰§è¡Œæµç¨‹ï¼š
1. Buildé˜¶æ®µï¼šæ„å»ºHashè¡¨ + ç”ŸæˆRuntime Filter
2. å°†Filterä¸‹æ¨åˆ°Probeç«¯çš„Scan
3. Probeé˜¶æ®µï¼šä½¿ç”¨Filterå‡å°‘æ‰«ææ•°æ®
```

### 7.2 Runtime Filterå®ç°

```cpp
class RuntimeFilterGenerator {
    // Join Buildé˜¶æ®µç”ŸæˆFilter
    RuntimeFilter* generateFilter(HashTable& hash_table) {
        if (hash_table.size() < MIN_FILTER_SIZE) {
            // å°è¡¨ä½¿ç”¨IN List Filter
            return new InListFilter(hash_table.getKeys());
        } else if (hash_table.size() < MAX_BLOOM_FILTER_SIZE) {
            // ä¸­ç­‰å¤§å°ä½¿ç”¨Bloom Filter
            BloomFilter* bf = new BloomFilter(hash_table.size());
            for (auto& key : hash_table) {
                bf->add(key);
            }
            return bf;
        } else {
            // å¤§è¡¨ä½¿ç”¨Min-Max Filter
            return new MinMaxFilter(
                hash_table.getMin(),
                hash_table.getMax()
            );
        }
    }
    
    // åº”ç”¨Runtime Filter
    bool applyFilter(RuntimeFilter* filter, Value value) {
        switch (filter->type()) {
            case IN_LIST:
                return static_cast<InListFilter*>(filter)->contains(value);
            case BLOOM_FILTER:
                return static_cast<BloomFilter*>(filter)->mayContain(value);
            case MIN_MAX:
                MinMaxFilter* mmf = static_cast<MinMaxFilter*>(filter);
                return value >= mmf->min && value <= mmf->max;
        }
    }
}
```

## å…«ã€å†…å­˜ç®¡ç†

### 8.1 å†…å­˜æ± ç®¡ç†

```cpp
// Chunkå†…å­˜æ± 
class ChunkMemoryPool {
private:
    struct ChunkBlock {
        char* data;
        size_t size;
        std::atomic<bool> in_use;
    };
    
    std::vector<ChunkBlock> blocks;
    std::mutex mutex;
    
public:
    Chunk* allocateChunk(size_t num_rows, Schema schema) {
        size_t required_size = calculateSize(num_rows, schema);
        
        // 1. å°è¯•å¤ç”¨å·²æœ‰å—
        for (auto& block : blocks) {
            if (!block.in_use && block.size >= required_size) {
                block.in_use = true;
                return new Chunk(block.data, num_rows, schema);
            }
        }
        
        // 2. åˆ†é…æ–°å—
        ChunkBlock new_block;
        new_block.data = static_cast<char*>(aligned_alloc(64, required_size));
        new_block.size = required_size;
        new_block.in_use = true;
        
        std::lock_guard<std::mutex> lock(mutex);
        blocks.push_back(new_block);
        
        return new Chunk(new_block.data, num_rows, schema);
    }
    
    void releaseChunk(Chunk* chunk) {
        // æ ‡è®°ä¸ºå¯å¤ç”¨
        for (auto& block : blocks) {
            if (block.data == chunk->getData()) {
                block.in_use = false;
                break;
            }
        }
    }
}
```

### 8.2 å†…å­˜æº¢å‡ºå¤„ç†

```cpp
// å¤§æ•°æ®é‡æ’åºçš„æº¢å‡ºå¤„ç†
class ExternalSort {
    void sort(std::vector<Chunk>& chunks) {
        size_t total_memory = getTotalMemory(chunks);
        
        if (total_memory <= memory_limit) {
            // å†…å­˜è¶³å¤Ÿï¼Œç›´æ¥æ’åº
            inMemorySort(chunks);
        } else {
            // å¤–éƒ¨æ’åº
            externalSort(chunks);
        }
    }
    
    void externalSort(std::vector<Chunk>& chunks) {
        // 1. åˆ†æ‰¹æ’åºå¹¶å†™å…¥ç£ç›˜
        std::vector<std::string> sorted_files;
        for (auto& batch : splitIntoBatches(chunks)) {
            inMemorySort(batch);
            std::string file = spillToDisk(batch);
            sorted_files.push_back(file);
        }
        
        // 2. å¤šè·¯å½’å¹¶
        multiWayMerge(sorted_files);
    }
}
```

## ä¹ã€æ‰§è¡Œç›‘æ§å’Œè°ƒè¯•

### 9.1 æ‰§è¡ŒæŒ‡æ ‡æ”¶é›†

```cpp
// ç®—å­æ‰§è¡Œç»Ÿè®¡
class OperatorMetrics {
    std::atomic<uint64_t> input_rows{0};
    std::atomic<uint64_t> output_rows{0};
    std::atomic<uint64_t> filtered_rows{0};
    std::atomic<uint64_t> execution_time_ns{0};
    std::atomic<uint64_t> memory_bytes{0};
    
    void recordExecution(uint64_t start_time, Chunk& input, Chunk& output) {
        input_rows += input.numRows();
        output_rows += output.numRows();
        filtered_rows += input.numRows() - output.numRows();
        execution_time_ns += getCurrentTime() - start_time;
        memory_bytes = getCurrentMemoryUsage();
    }
    
    std::string toString() {
        return fmt::format(
            "InputRows: {}, OutputRows: {}, FilteredRows: {}, "
            "Time: {}ms, Memory: {}MB, FilterRatio: {:.2f}%",
            input_rows.load(),
            output_rows.load(),
            filtered_rows.load(),
            execution_time_ns.load() / 1000000,
            memory_bytes.load() / 1048576,
            filtered_rows * 100.0 / input_rows
        );
    }
}
```

### 9.2 Profileè¾“å‡ºç¤ºä¾‹

```
Query Profile: query_id=20240116_123456_00001
Total Time: 523ms

Fragment 0 (Coordinator)
  â””â”€ RESULT_SINK (id=14)
      â”‚ output: 100 rows
      â”‚ time: 2ms
      â””â”€ TOP-N (id=13)
          â”‚ limit: 100
          â”‚ sort keys: total_revenue DESC
          â”‚ input: 1,250 rows, output: 100 rows
          â”‚ time: 15ms, memory: 2MB
          â””â”€ EXCHANGE (id=12) [GATHER]
              â”‚ hosts: 3
              â”‚ input: 1,250 rows
              â”‚ network: 145KB, time: 8ms

Fragment 1 (Aggregation)
  Pipeline 1:
    â””â”€ AGGREGATE (id=11)
        â”‚ group by: city, category, brand
        â”‚ aggregations: COUNT(DISTINCT user_id), SUM(amount)
        â”‚ input: 45,230 rows, output: 1,250 rows
        â”‚ time: 85ms, memory: 32MB
        â”‚ hash table size: 1,250, load factor: 0.75
        â””â”€ HASH_JOIN (id=10) [INNER]
            â”‚ join keys: om.user_id = u.user_id
            â”‚ runtime filter: RF001 (bloom filter)
            â”‚ input: 125,450 rows, output: 45,230 rows
            â”‚ time: 125ms, memory: 128MB
            â”‚ build rows: 15,230, probe rows: 125,450
            â”œâ”€ EXCHANGE (id=9) [SHUFFLE by user_id]
            â”‚   â”‚ input: 125,450 rows
            â”‚   â”‚ partitions: 32
            â”‚   â””â”€ [Fragment 2]
            â””â”€ EXCHANGE (id=8) [SHUFFLE by user_id]
                â”‚ input: 15,230 rows
                â””â”€ [Fragment 3]

Fragment 2 (Order Processing)
  Pipeline 1:
    â””â”€ WINDOW_FUNCTION (id=7)
        â”‚ function: ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY amount DESC)
        â”‚ input: 235,680 rows, output: 235,680 rows
        â”‚ time: 178ms, memory: 256MB
        â””â”€ HASH_JOIN (id=6) [INNER]
            â”‚ join keys: o.product_id = p.product_id
            â”‚ type: BROADCAST
            â”‚ input: 458,230 rows, output: 235,680 rows
            â”‚ time: 95ms, memory: 45MB
            â”œâ”€ OLAP_SCAN (id=5) [orders]
            â”‚   â”‚ tablets: 32/32
            â”‚   â”‚ predicates: order_date BETWEEN '2024-01-01' AND '2024-01-31'
            â”‚   â”‚              AND status = 'COMPLETED'
            â”‚   â”‚ runtime filters: RF001, RF002
            â”‚   â”‚ rows: 458,230/1,250,000 (36.66%)
            â”‚   â”‚ time: 125ms
            â””â”€ EXCHANGE (id=4) [BROADCAST]
                â”‚ input: 10,000 rows
                â””â”€ [Fragment 4]

Performance Analysis:
- Bottleneck: Window Function in Fragment 2 (178ms, 34% of total)
- High Memory Usage: Fragment 2 Window Function (256MB)
- Good Filter Effectiveness: Runtime Filter reduced 63.34% rows
- Network Transfer: Total 2.3MB across fragments
- Parallelism: DOP=32 for scan, DOP=16 for join
```

## åã€æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 10.1 æ‰§è¡Œè®¡åˆ’ä¼˜åŒ–

```sql
-- 1. ä½¿ç”¨é€‚å½“çš„Hintæ§åˆ¶æ‰§è¡Œç­–ç•¥
SELECT /*+ SET_VAR(pipeline_dop=64) */  -- å¢åŠ å¹¶è¡Œåº¦
       /*+ BROADCAST(small_table) */     -- å¼ºåˆ¶å¹¿æ’­å°è¡¨
       /*+ LEADING(large_table, medium_table, small_table) */ -- æ§åˆ¶Joiné¡ºåº
    *
FROM large_table
JOIN medium_table ON ...
JOIN small_table ON ...;

-- 2. åˆ©ç”¨Runtime Filterä¼˜åŒ–
SET runtime_filter_type = 'IN,BLOOM_FILTER,MIN_MAX';
SET runtime_filter_wait_time_ms = 200;  -- ç­‰å¾…Runtime Filterçš„æ—¶é—´

-- 3. å†…å­˜ä¼˜åŒ–
SET exec_mem_limit = 8GB;  -- å•æŸ¥è¯¢å†…å­˜é™åˆ¶
SET chunk_size = 4096;      -- Chunkå¤§å°ï¼ˆå½±å“å‘é‡åŒ–æ•ˆç‡ï¼‰
```

### 10.2 Pipelineä¼˜åŒ–

```sql
-- Pipelineç›¸å…³é…ç½®
SET pipeline_dop = 0;  -- 0è¡¨ç¤ºè‡ªåŠ¨ï¼Œ>0è¡¨ç¤ºå›ºå®šå¹¶è¡Œåº¦
SET enable_pipeline_engine = true;
SET pipeline_profile_level = 2;  -- Profileè¯¦ç»†ç¨‹åº¦

-- æŸ¥çœ‹Pipelineæ‰§è¡Œè¯¦æƒ…
EXPLAIN PIPELINE
SELECT ...;

-- ç›‘æ§Pipelineæ‰§è¡Œ
SELECT * FROM information_schema.pipeline_metrics
WHERE query_id = 'xxx';
```

### 10.3 å‘é‡åŒ–ä¼˜åŒ–

```sql
-- ç¡®ä¿å‘é‡åŒ–æ‰§è¡Œ
SET enable_vectorized_engine = true;
SET batch_size = 4096;  -- å‘é‡åŒ–æ‰¹æ¬¡å¤§å°

-- æ£€æŸ¥å‘é‡åŒ–æ˜¯å¦ç”Ÿæ•ˆ
EXPLAIN VERBOSE
SELECT ...;
-- æŸ¥çœ‹æ˜¯å¦æœ‰"vectorized: true"æ ‡è®°
```

## åä¸€ã€æ‰§è¡Œå¼•æ“æ¼”è¿›

### 11.1 ç‰ˆæœ¬æ¼”è¿›å†ç¨‹

| ç‰ˆæœ¬ | æ‰§è¡Œå¼•æ“ç‰¹æ€§ | ä¸»è¦æ”¹è¿› |
|------|------------|---------|
| 1.x | ç«å±±æ¨¡å‹ | åŸºç¡€çš„pullæ¨¡å‹æ‰§è¡Œ |
| 2.0 | å‘é‡åŒ–æ‰§è¡Œ | æ‰¹é‡å¤„ç†ï¼ŒSIMDä¼˜åŒ– |
| 2.5 | Pipelineå¼•æ“ | æ¨æ¨¡å‹ï¼Œæ›´å¥½çš„å¹¶è¡Œæ€§ |
| 3.0 | è‡ªé€‚åº”æ‰§è¡Œ | åŠ¨æ€è°ƒæ•´å¹¶è¡Œåº¦å’Œå†…å­˜ |
| 3.1 | Morsel-driven | ç»†ç²’åº¦çš„ä»»åŠ¡è°ƒåº¦ |
| 3.2+ | Async Pipeline | å¼‚æ­¥I/Oï¼Œåç¨‹è°ƒåº¦ |

### 11.2 æœªæ¥å‘å±•æ–¹å‘

```
1. GPUåŠ é€Ÿ
   - GPUæ‰§è¡Œç®—å­
   - GPUå†…å­˜ç®¡ç†
   
2. è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ(AQE)
   - åŠ¨æ€åˆ†åŒºè£å‰ª
   - åŠ¨æ€Joinç­–ç•¥è°ƒæ•´
   - è‡ªåŠ¨å€¾æ–œå¤„ç†
   
3. å‘é‡åŒ–2.0
   - æ›´æ¿€è¿›çš„å‘é‡åŒ–
   - è‡ªå®šä¹‰SIMDä»£ç ç”Ÿæˆ
   
4. æ™ºèƒ½è°ƒåº¦
   - æœºå™¨å­¦ä¹ é©±åŠ¨çš„ä¼˜åŒ–
   - å†å²æŸ¥è¯¢patternå­¦ä¹ 
```

## åäºŒã€å®æˆ˜æ¡ˆä¾‹åˆ†æ

### 12.1 æ¡ˆä¾‹1ï¼šå¤§è¡¨Joinä¼˜åŒ–

```sql
-- é—®é¢˜ï¼šä¸¤ä¸ª10äº¿è¡Œè¡¨Joinï¼Œæ‰§è¡Œè¶…æ—¶
-- åŸå§‹SQL
SELECT a.*, b.*
FROM billion_table_a a
JOIN billion_table_b b ON a.key = b.key
WHERE a.date = '2024-01-15';

-- ä¼˜åŒ–æ–¹æ¡ˆ1ï¼šåˆ©ç”¨åˆ†åŒºè£å‰ª + Runtime Filter
ALTER TABLE billion_table_a 
PARTITION BY RANGE(date) (
    PARTITION p20240115 VALUES [('2024-01-15'), ('2024-01-16'))
);

-- ä¼˜åŒ–æ–¹æ¡ˆ2ï¼šä½¿ç”¨Colocation Join
ALTER TABLE billion_table_a 
SET ("colocate_with" = "group1");
ALTER TABLE billion_table_b 
SET ("colocate_with" = "group1");

-- ä¼˜åŒ–æ–¹æ¡ˆ3ï¼šç‰©åŒ–ä¸­é—´ç»“æœ
CREATE MATERIALIZED VIEW mv_daily_join AS
SELECT a.key, a.col1, b.col2, a.date
FROM billion_table_a a
JOIN billion_table_b b ON a.key = b.key;

-- ä¼˜åŒ–åæŸ¥è¯¢
SELECT * FROM mv_daily_join WHERE date = '2024-01-15';
```

### 12.2 æ¡ˆä¾‹2ï¼šçª—å£å‡½æ•°æ€§èƒ½ä¼˜åŒ–

```sql
-- é—®é¢˜ï¼šçª—å£å‡½æ•°å¯¼è‡´å†…å­˜æº¢å‡º
-- åŸå§‹SQL
SELECT 
    user_id,
    order_time,
    amount,
    SUM(amount) OVER (PARTITION BY user_id ORDER BY order_time 
                      ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as cumsum
FROM orders;

-- ä¼˜åŒ–æ–¹æ¡ˆ1ï¼šåˆ†æ‰¹å¤„ç†
WITH user_batches AS (
    SELECT DISTINCT user_id, MOD(HASH(user_id), 100) as batch_id
    FROM orders
)
SELECT /*+ SET_VAR(exec_mem_limit=2GB) */
    o.user_id,
    o.order_time,
    o.amount,
    SUM(o.amount) OVER (PARTITION BY o.user_id ORDER BY o.order_time) as cumsum
FROM orders o
JOIN user_batches ub ON o.user_id = ub.user_id
WHERE ub.batch_id = ?;  -- åˆ†100æ‰¹æ‰§è¡Œ

-- ä¼˜åŒ–æ–¹æ¡ˆ2ï¼šä½¿ç”¨segment treeç‰©åŒ–è§†å›¾
CREATE MATERIALIZED VIEW mv_cumsum AS
SELECT 
    user_id,
    DATE_TRUNC('day', order_time) as day,
    SUM(amount) as daily_amount,
    SUM(SUM(amount)) OVER (PARTITION BY user_id ORDER BY DATE_TRUNC('day', order_time)) as cumsum
FROM orders
GROUP BY user_id, DATE_TRUNC('day', order_time);
```

## åä¸‰ã€æ•…éšœè¯Šæ–­æŒ‡å—

### 13.1 å¸¸è§æ‰§è¡Œé—®é¢˜

```sql
-- 1. æŸ¥è¯¢hangä½
-- æ£€æŸ¥æ˜¯å¦åœ¨ç­‰å¾…Runtime Filter
SHOW PROCESSLIST;
-- æŸ¥çœ‹æ˜¯å¦æœ‰ "Wait for runtime filter" çŠ¶æ€

-- è§£å†³ï¼šå‡å°‘ç­‰å¾…æ—¶é—´
SET runtime_filter_wait_time_ms = 100;

-- 2. å†…å­˜æº¢å‡º
-- æŸ¥çœ‹å†…å­˜ä½¿ç”¨
SELECT * FROM information_schema.mem_tracker
WHERE query_id = 'xxx';

-- è§£å†³ï¼šå¢åŠ å†…å­˜æˆ–ä¼˜åŒ–æŸ¥è¯¢
SET exec_mem_limit = 16GB;
SET spill_enable = true;  -- å¯ç”¨æº¢å‡ºåˆ°ç£ç›˜

-- 3. æ•°æ®å€¾æ–œ
-- æ£€æµ‹å€¾æ–œ
SELECT 
    fragment_instance_id,
    rows_processed,
    execution_time_ms
FROM information_schema.fragment_instances
WHERE query_id = 'xxx'
ORDER BY rows_processed DESC;

-- è§£å†³ï¼šä½¿ç”¨éšæœºå‰ç¼€æ‰“æ•£
SELECT ...
FROM table
JOIN (
    SELECT *, RAND() % 10 as salt
    FROM skewed_table
) t ON ...
```

### 13.2 Profileåˆ†ææŠ€å·§

```python
# åˆ†æProfileçš„Pythonè„šæœ¬
import json
import pandas as pd

def analyze_profile(profile_json):
    """åˆ†æStarRocksæŸ¥è¯¢Profile"""
    profile = json.loads(profile_json)
    
    # 1. æ‰¾å‡ºæœ€è€—æ—¶çš„ç®—å­
    operators = []
    for fragment in profile['fragments']:
        for operator in fragment['operators']:
            operators.append({
                'fragment_id': fragment['id'],
                'operator': operator['name'],
                'time_ms': operator['time_ms'],
                'rows': operator['output_rows'],
                'memory_mb': operator['memory_bytes'] / 1048576
            })
    
    df = pd.DataFrame(operators)
    print("Top 5 è€—æ—¶ç®—å­:")
    print(df.nlargest(5, 'time_ms'))
    
    # 2. åˆ†ææ•°æ®è¿‡æ»¤æ•ˆæœ
    scan_rows = df[df['operator'].str.contains('SCAN')]['rows'].sum()
    final_rows = df.iloc[-1]['rows']
    filter_rate = (scan_rows - final_rows) / scan_rows * 100
    print(f"\næ•°æ®è¿‡æ»¤ç‡: {filter_rate:.2f}%")
    
    # 3. æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®å€¾æ–œ
    if 'instances' in profile:
        instance_rows = [inst['rows'] for inst in profile['instances']]
        cv = np.std(instance_rows) / np.mean(instance_rows)
        if cv > 0.5:
            print(f"\nè­¦å‘Šï¼šæ£€æµ‹åˆ°æ•°æ®å€¾æ–œï¼ŒCV={cv:.2f}")
    
    return df
```

## æ€»ç»“

é€šè¿‡è¿™ä¸ªå¤æ‚SQLçš„æ‰§è¡Œè¿‡ç¨‹ï¼Œæˆ‘ä»¬æ·±å…¥äº†è§£äº†StarRocksæ‰§è¡Œå¼•æ“çš„æ ¸å¿ƒç»„ä»¶ï¼š

1. **æŸ¥è¯¢ä¼˜åŒ–**ï¼šé€»è¾‘ä¼˜åŒ–å’ŒåŸºäºæˆæœ¬çš„ç‰©ç†ä¼˜åŒ–
2. **Fragmentåˆ’åˆ†**ï¼šå°†æŸ¥è¯¢åˆ’åˆ†ä¸ºå¯å¹¶è¡Œæ‰§è¡Œçš„ç‰‡æ®µ
3. **Pipelineæ¨¡å‹**ï¼šæ¨å¼æ‰§è¡Œï¼Œæé«˜CPUåˆ©ç”¨ç‡
4. **å‘é‡åŒ–æ‰§è¡Œ**ï¼šæ‰¹é‡å¤„ç†å’ŒSIMDä¼˜åŒ–
5. **Runtime Filter**ï¼šåŠ¨æ€è¿‡æ»¤ï¼Œå‡å°‘æ•°æ®ä¼ è¾“
6. **å†…å­˜ç®¡ç†**ï¼šå†…å­˜æ± å’Œæº¢å‡ºå¤„ç†
7. **æ‰§è¡Œç›‘æ§**ï¼šè¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡å’ŒProfile

ç†è§£è¿™äº›åº•å±‚åŸç†ï¼Œæœ‰åŠ©äºï¼š
- ç¼–å†™é«˜æ•ˆçš„SQLæŸ¥è¯¢
- æ­£ç¡®è§£è¯»æ‰§è¡Œè®¡åˆ’
- å¿«é€Ÿå®šä½æ€§èƒ½ç“¶é¢ˆ
- è¿›è¡Œé’ˆå¯¹æ€§çš„ä¼˜åŒ–

StarRocksçš„æ‰§è¡Œå¼•æ“è¿˜åœ¨æŒç»­æ¼”è¿›ï¼Œå‘ç€æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆçš„æ–¹å‘å‘å±•ã€‚

---
## ğŸ“– å¯¼èˆª
[ğŸ  è¿”å›ä¸»é¡µ](../../README.md) | [â¬…ï¸ ä¸Šä¸€é¡µ](big-data-ecosystem.md) | [â¡ï¸ ä¸‹ä¸€é¡µ](../07-best-practices/oracle-migration-best-practices.md)
---