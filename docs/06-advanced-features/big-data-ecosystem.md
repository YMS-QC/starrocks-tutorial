---
## ğŸ“– å¯¼èˆª
[ğŸ  è¿”å›ä¸»é¡µ](../../README.md) | [â¬…ï¸ ä¸Šä¸€é¡µ](cloud-native-architecture.md) | [â¡ï¸ ä¸‹ä¸€é¡µ](../07-best-practices/production-deployment.md)
---

# å¤§æ•°æ®ç”Ÿæ€é›†æˆ

StarRocks ä½œä¸ºç°ä»£åŒ–çš„ MPP æ•°æ®åº“ï¼Œæä¾›äº†ä¸°å¯Œçš„å¤§æ•°æ®ç”Ÿæ€é›†æˆèƒ½åŠ›ï¼Œæ”¯æŒä¸ Hadoopã€Sparkã€Flinkã€Kafka ç­‰ä¸»æµå¤§æ•°æ®ç»„ä»¶çš„æ— ç¼é›†æˆã€‚æœ¬ç« èŠ‚ä»‹ç»ä¸»è¦çš„é›†æˆæ–¹æ¡ˆå’Œæœ€ä½³å®è·µã€‚

## 1. å¤§æ•°æ®ç”Ÿæ€æ¦‚è§ˆ

### 1.1 é›†æˆæ¶æ„å›¾

```
æ•°æ®æºå±‚
â”œâ”€â”€ MySQL/Oracle (OLTP)
â”œâ”€â”€ Kafka (æµæ•°æ®)  
â”œâ”€â”€ HDFS/S3 (å¯¹è±¡å­˜å‚¨)
â””â”€â”€ Hive (æ•°æ®æ¹–)

ETLå¤„ç†å±‚
â”œâ”€â”€ Flink (å®æ—¶è®¡ç®—)
â”œâ”€â”€ Spark (æ‰¹å¤„ç†)
â”œâ”€â”€ DataX (æ•°æ®åŒæ­¥)
â””â”€â”€ Kettle (ETLå·¥å…·)

å­˜å‚¨è®¡ç®—å±‚
â”œâ”€â”€ StarRocks (OLAPåˆ†æ)
â”œâ”€â”€ Hive (æ•°æ®ä»“åº“)
â”œâ”€â”€ HBase (KVå­˜å‚¨)
â””â”€â”€ ClickHouse (æ—¶åºåˆ†æ)

åº”ç”¨æœåŠ¡å±‚
â”œâ”€â”€ BIå·¥å…· (Tableau, PowerBI)
â”œâ”€â”€ æ•°æ®ç§‘å­¦ (Jupyter, PyTorch)  
â”œâ”€â”€ APIæœåŠ¡ (RESTful)
â””â”€â”€ å®æ—¶å¤§å± (Grafana)
```

### 1.2 é›†æˆæ–¹å¼åˆ†ç±»

| é›†æˆç±»å‹ | æŠ€æœ¯æ–¹æ¡ˆ | é€‚ç”¨åœºæ™¯ | æ€§èƒ½ç‰¹ç‚¹ |
|---------|----------|----------|----------|
| **å¤–éƒ¨è¡¨** | External Table | æŸ¥è¯¢å¤–éƒ¨æ•°æ®ï¼Œæ— éœ€å¯¼å…¥ | å®æ—¶æ€§å¥½ï¼Œæ€§èƒ½ä¸€èˆ¬ |
| **æ•°æ®å¯¼å…¥** | Stream Load, Routine Load | é«˜é¢‘å†™å…¥ï¼Œæ•°æ®æœ¬åœ°åŒ– | é«˜æ€§èƒ½ï¼Œå»¶è¿Ÿä½ |
| **è”é‚¦æŸ¥è¯¢** | Catalog æœºåˆ¶ | è·¨ç³»ç»ŸæŸ¥è¯¢åˆ†æ | çµæ´»æ€§é«˜ï¼Œæ€§èƒ½ä¸­ç­‰ |
| **æ‰¹é‡åŒæ­¥** | å®šæ—¶ä»»åŠ¡ï¼Œå…¨é‡/å¢é‡ | å†å²æ•°æ®è¿ç§» | ååé‡å¤§ï¼Œå»¶è¿Ÿé«˜ |

## 2. External Table é›†æˆ

### 2.1 Hive External Table

**åŸºç¡€é…ç½®**
```sql
-- åˆ›å»º Hive Resource
CREATE EXTERNAL RESOURCE hive_resource
PROPERTIES (
    "type" = "hive",
    "hive.metastore.uris" = "thrift://hive-metastore:9083"
);

-- åˆ›å»º Hive Catalog
CREATE EXTERNAL CATALOG hive_catalog 
PROPERTIES (
    "type" = "hive",
    "hive.metastore.uris" = "thrift://hive-metastore:9083"
);

-- ç›´æ¥æŸ¥è¯¢ Hive è¡¨
SELECT 
    order_date,
    COUNT(*) as order_count,
    SUM(amount) as total_amount
FROM hive_catalog.warehouse.orders
WHERE order_date >= '2023-01-01'
GROUP BY order_date
ORDER BY order_date;
```

**é«˜çº§é…ç½®**
```sql
-- é…ç½® Hive è®¤è¯
CREATE EXTERNAL CATALOG secure_hive_catalog
PROPERTIES (
    "type" = "hive", 
    "hive.metastore.uris" = "thrift://hive-metastore:9083",
    "hadoop.security.authentication" = "kerberos",
    "hadoop.kerberos.keytab" = "/path/to/keytab",
    "hadoop.kerberos.principal" = "starrocks@REALM.COM"
);

-- é…ç½® S3 è®¿é—®
CREATE EXTERNAL CATALOG s3_hive_catalog
PROPERTIES (
    "type" = "hive",
    "hive.metastore.uris" = "thrift://hive-metastore:9083",
    "aws.s3.access_key" = "your-access-key",
    "aws.s3.secret_key" = "your-secret-key",
    "aws.s3.endpoint" = "s3.amazonaws.com",
    "aws.s3.region" = "us-west-2"
);
```

**æ€§èƒ½ä¼˜åŒ–**
```sql
-- ä½¿ç”¨åˆ†åŒºè£å‰ª
SELECT * FROM hive_catalog.warehouse.partitioned_table
WHERE partition_date = '2023-01-01';  -- åªæ‰«æç‰¹å®šåˆ†åŒº

-- åˆ—è£å‰ªä¼˜åŒ–
SELECT user_id, amount FROM hive_catalog.warehouse.orders  -- åªè¯»å–éœ€è¦çš„åˆ—
WHERE order_date >= '2023-01-01';

-- é¢„èšåˆä¸‹æ¨
SELECT 
    order_date,
    COUNT(*) as cnt
FROM hive_catalog.warehouse.orders
WHERE order_date >= '2023-01-01'
GROUP BY order_date;  -- èšåˆæ“ä½œä¸‹æ¨åˆ° Hive
```

### 2.2 Iceberg é›†æˆ

```sql
-- åˆ›å»º Iceberg Catalog
CREATE EXTERNAL CATALOG iceberg_catalog
PROPERTIES (
    "type" = "iceberg",
    "iceberg.catalog.type" = "hive",
    "hive.metastore.uris" = "thrift://hive-metastore:9083"
);

-- æŸ¥è¯¢ Iceberg è¡¨
SELECT 
    snapshot_id,
    committed_at,
    summary
FROM iceberg_catalog.db.table_snapshots
ORDER BY committed_at DESC;

-- æ—¶é—´æ—…è¡ŒæŸ¥è¯¢
SELECT * FROM iceberg_catalog.warehouse.orders
FOR SYSTEM_TIME AS OF '2023-01-01 00:00:00';

-- å¢é‡æŸ¥è¯¢
SELECT * FROM iceberg_catalog.warehouse.orders
FOR SYSTEM_VERSION AS OF 12345;
```

### 2.3 Delta Lake é›†æˆ

```sql
-- åˆ›å»º Delta Lake Catalog  
CREATE EXTERNAL CATALOG delta_catalog
PROPERTIES (
    "type" = "deltalake",
    "hive.metastore.uris" = "thrift://hive-metastore:9083"
);

-- æŸ¥è¯¢ Delta è¡¨
SELECT 
    _delta_log_version,
    _delta_log_timestamp,
    COUNT(*) as record_count
FROM delta_catalog.warehouse.orders
GROUP BY _delta_log_version, _delta_log_timestamp
ORDER BY _delta_log_version DESC;
```

## 3. æµå¼æ•°æ®é›†æˆ

### 3.1 Kafka é›†æˆ

**Routine Load é…ç½®**
```sql
-- åˆ›å»º Kafka Routine Load
CREATE ROUTINE LOAD routine_load_orders ON orders
PROPERTIES (
    "desired_concurrent_number" = "5",
    "max_batch_interval" = "10",
    "max_batch_rows" = "250000",
    "max_error_number" = "1000",
    "strict_mode" = "false",
    "timezone" = "Asia/Shanghai"
)
FROM KAFKA (
    "kafka_broker_list" = "kafka1:9092,kafka2:9092,kafka3:9092",
    "kafka_topic" = "orders_topic",
    "kafka_partitions" = "0,1,2,3,4,5",
    "property.group.id" = "starrocks_group",
    "property.client.id" = "starrocks_client",
    "property.kafka_default_offsets" = "OFFSET_BEGINNING"
);

-- æŸ¥çœ‹ Routine Load çŠ¶æ€
SHOW ROUTINE LOAD FOR routine_load_orders;

-- æš‚åœå’Œæ¢å¤
PAUSE ROUTINE LOAD FOR routine_load_orders;
RESUME ROUTINE LOAD FOR routine_load_orders;
```

**JSON æ•°æ®å¤„ç†**
```sql
-- å¤„ç† JSON æ ¼å¼çš„ Kafka æ¶ˆæ¯
CREATE ROUTINE LOAD json_routine_load ON user_events
COLUMNS (
    user_id,
    event_type,
    event_time,
    properties = JSON_EXTRACT(message, '$.properties')
)
PROPERTIES (
    "format" = "json",
    "jsonpaths" = "[\"$.user_id\", \"$.event_type\", \"$.event_time\", \"$.properties\"]"
)
FROM KAFKA (
    "kafka_broker_list" = "kafka:9092",
    "kafka_topic" = "user_events",
    "property.group.id" = "starrocks_events_group"
);
```

**é”™è¯¯å¤„ç†å’Œç›‘æ§**
```sql
-- æŸ¥çœ‹é”™è¯¯ä¿¡æ¯
SHOW ROUTINE LOAD TASK WHERE job_name = 'routine_load_orders';

-- åˆ›å»ºé”™è¯¯ç›‘æ§è§†å›¾
CREATE VIEW routine_load_monitor AS
SELECT 
    job_name,
    state,
    consume_rate,
    error_rate,
    committed_task_num,
    abort_task_num
FROM information_schema.routine_loads;

-- è®¾ç½®é”™è¯¯é˜ˆå€¼å‘Šè­¦
SELECT * FROM routine_load_monitor 
WHERE error_rate > 0.01 OR state != 'RUNNING';
```

### 3.2 Flink é›†æˆ

**Flink StarRocks Connector**
```java
// Flink ä½œä¸šé…ç½®
public class FlinkStarRocksJob {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        
        // é…ç½® StarRocks Sink
        StarRocksSink.Builder<String> builder = StarRocksSink.builder()
            .withProperty("fenodes", "starrocks-fe:8030")
            .withProperty("username", "root")
            .withProperty("password", "")
            .withProperty("table-name", "orders")
            .withProperty("database-name", "warehouse")
            .withProperty("sink.properties.format", "json")
            .withProperty("sink.properties.strip_outer_array", "true");
            
        // ä» Kafka è¯»å–æ•°æ®
        DataStream<String> kafkaStream = env
            .addSource(new FlinkKafkaConsumer<>("orders_topic", 
                new SimpleStringSchema(), properties))
            .name("kafka-source");
            
        // æ•°æ®è½¬æ¢å¤„ç†
        DataStream<String> processedStream = kafkaStream
            .map(new OrderTransformFunction())
            .name("transform");
            
        // å†™å…¥ StarRocks
        processedStream.sinkTo(builder.build())
            .name("starrocks-sink");
            
        env.execute("Flink-StarRocks-Job");
    }
}
```

**å®æ—¶æ•°æ®å¤„ç†**
```java
// å®æ—¶çª—å£èšåˆ
public class RealtimeAggregationJob {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        
        DataStream<Order> orderStream = env
            .addSource(new FlinkKafkaConsumer<>(...))
            .map(new JsonToOrderFunction());
            
        // æ»šåŠ¨çª—å£èšåˆ
        DataStream<OrderSummary> aggregated = orderStream
            .keyBy(Order::getUserId)
            .window(TumblingProcessingTimeWindows.of(Time.minutes(5)))
            .aggregate(new OrderAggregateFunction());
            
        // å®æ—¶å†™å…¥ StarRocks
        aggregated.sinkTo(StarRocksSink.builder()
            .withProperty("table-name", "realtime_order_summary")
            .build());
            
        env.execute();
    }
}
```

## 4. æ‰¹å¤„ç†é›†æˆ

### 4.1 Spark é›†æˆ

**Spark StarRocks Connector**
```scala
// Spark æ‰¹å¤„ç†ä½œä¸š
import org.apache.spark.sql.SparkSession

object SparkStarRocksJob {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("Spark-StarRocks-ETL")
      .getOrCreate()
      
    // ä» Hive è¯»å–æ•°æ®
    val sourceData = spark.sql("""
      SELECT 
        user_id,
        product_id, 
        order_date,
        amount
      FROM hive_warehouse.raw_orders
      WHERE order_date >= '2023-01-01'
    """)
    
    // æ•°æ®æ¸…æ´—å’Œè½¬æ¢
    val cleanedData = sourceData
      .filter($"amount" > 0)
      .withColumn("order_month", date_format($"order_date", "yyyy-MM"))
      .groupBy("user_id", "order_month")
      .agg(
        count("*").alias("order_count"),
        sum("amount").alias("total_amount")
      )
    
    // å†™å…¥ StarRocks
    cleanedData.write
      .format("starrocks")
      .option("starrocks.fe.http.url", "starrocks-fe:8030")
      .option("starrocks.fe.jdbc.url", "jdbc:mysql://starrocks-fe:9030")
      .option("starrocks.table.identifier", "warehouse.user_monthly_summary")
      .option("starrocks.user", "root")
      .option("starrocks.password", "")
      .mode("append")
      .save()
  }
}
```

**DataFrame API ä½¿ç”¨**
```scala
// ä½¿ç”¨ DataFrame API è¿›è¡Œå¤æ‚ ETL
val orders = spark.read
  .format("starrocks")
  .option("starrocks.table.identifier", "warehouse.orders")
  .option("starrocks.read.field", "order_id,user_id,amount,order_date")
  .load()

val users = spark.read
  .format("starrocks") 
  .option("starrocks.table.identifier", "warehouse.users")
  .load()

// å¤æ‚çš„ Join å’Œèšåˆ
val result = orders
  .join(users, "user_id")
  .filter($"order_date" >= "2023-01-01")
  .groupBy($"user_region", $"user_level")
  .agg(
    count("order_id").alias("total_orders"),
    sum("amount").alias("total_revenue"),
    avg("amount").alias("avg_order_value")
  )
  
result.write
  .format("starrocks")
  .option("starrocks.table.identifier", "warehouse.region_summary")
  .mode("overwrite")
  .save()
```

### 4.2 DataX é›†æˆ

**DataX ä½œä¸šé…ç½®**
```json
{
    "job": {
        "setting": {
            "speed": {
                "channel": 5
            },
            "errorLimit": {
                "record": 1000,
                "percentage": 0.1
            }
        },
        "content": [{
            "reader": {
                "name": "mysqlreader",
                "parameter": {
                    "username": "mysql_user",
                    "password": "mysql_password",
                    "column": ["order_id", "user_id", "amount", "order_date"],
                    "splitPk": "order_id",
                    "connection": [{
                        "table": ["orders"],
                        "jdbcUrl": ["jdbc:mysql://mysql:3306/warehouse"]
                    }],
                    "where": "order_date >= '2023-01-01'"
                }
            },
            "writer": {
                "name": "starrockswriter", 
                "parameter": {
                    "username": "root",
                    "password": "",
                    "database": "warehouse",
                    "table": "orders",
                    "column": ["order_id", "user_id", "amount", "order_date"],
                    "preSql": [],
                    "postSql": [],
                    "jdbcUrl": "jdbc:mysql://starrocks-fe:9030/warehouse",
                    "loadUrl": ["starrocks-fe:8030"],
                    "loadProps": {
                        "format": "json",
                        "max_filter_ratio": "0.1"
                    }
                }
            }
        }]
    }
}
```

## 5. BI å·¥å…·é›†æˆ

### 5.1 Tableau é›†æˆ

**è¿æ¥é…ç½®**
```sql
-- åˆ›å»º Tableau ä¸“ç”¨ç”¨æˆ·
CREATE USER tableau_user IDENTIFIED BY 'tableau_password';
GRANT SELECT_PRIV ON warehouse.* TO tableau_user;

-- ä¼˜åŒ– Tableau æŸ¥è¯¢çš„è¡¨è®¾è®¡
CREATE TABLE tableau_sales_summary (
    date_key DATE,
    region_name VARCHAR(100),
    product_category VARCHAR(100), 
    sales_amount DECIMAL(15,2),
    order_count BIGINT,
    unique_customers BIGINT
) ENGINE=OLAP
DUPLICATE KEY(date_key, region_name, product_category)
PARTITION BY RANGE(date_key) (
    -- åˆ†åŒºé…ç½®
)
DISTRIBUTED BY HASH(region_name) BUCKETS 16
ORDER BY (date_key, region_name, product_category);
```

**æ€§èƒ½ä¼˜åŒ–è§†å›¾**
```sql
-- ä¸º Tableau åˆ›å»ºé¢„èšåˆè§†å›¾
CREATE VIEW tableau_dashboard_data AS
SELECT 
    DATE(order_date) as order_date,
    u.user_region,
    p.product_category,
    COUNT(*) as order_count,
    SUM(o.amount) as total_revenue,
    COUNT(DISTINCT o.user_id) as unique_customers,
    AVG(o.amount) as avg_order_value
FROM orders o
JOIN users u ON o.user_id = u.user_id
JOIN products p ON o.product_id = p.product_id
WHERE o.order_date >= CURRENT_DATE - INTERVAL 90 DAY
GROUP BY DATE(o.order_date), u.user_region, p.product_category;
```

### 5.2 Superset é›†æˆ

**æ•°æ®æºé…ç½®**
```python
# Superset æ•°æ®åº“è¿æ¥é…ç½®
DATABASES = {
    'starrocks': {
        'ENGINE': 'django.db.backends.mysql',
        'NAME': 'warehouse',
        'USER': 'superset_user',
        'PASSWORD': 'superset_password', 
        'HOST': 'starrocks-fe',
        'PORT': '9030',
        'OPTIONS': {
            'charset': 'utf8mb4',
        }
    }
}
```

**è‡ªå®šä¹‰ SQL æŸ¥è¯¢**
```sql
-- Superset è‡ªå®šä¹‰æ•°æ®é›†
SELECT 
    DATE_FORMAT(order_date, '%Y-%m') as order_month,
    user_region,
    SUM(amount) as monthly_revenue,
    COUNT(DISTINCT user_id) as monthly_active_users,
    COUNT(*) as monthly_orders
FROM orders o
JOIN users u ON o.user_id = u.user_id
WHERE order_date >= DATE_SUB(CURRENT_DATE, INTERVAL 12 MONTH)
GROUP BY DATE_FORMAT(order_date, '%Y-%m'), user_region
ORDER BY order_month DESC, user_region;
```

## 6. äº‘åŸç”Ÿé›†æˆ

### 6.1 Kubernetes éƒ¨ç½²

**StarRocks Operator**
```yaml
# starrocks-cluster.yaml
apiVersion: starrocks.com/v1alpha1
kind: StarRocksCluster
metadata:
  name: starrocks-cluster
  namespace: starrocks
spec:
  starRocksFeSpec:
    image: starrocks/fe-ubuntu:latest
    replicas: 3
    resources:
      requests:
        cpu: "4"
        memory: "8Gi"
      limits:
        cpu: "8" 
        memory: "16Gi"
    service:
      type: LoadBalancer
      
  starRocksBeSpec:
    image: starrocks/be-ubuntu:latest
    replicas: 6
    resources:
      requests:
        cpu: "8"
        memory: "32Gi"
      limits:
        cpu: "16"
        memory: "64Gi"
    storageSpec:
      storageClassName: ssd
      storageSize: 1000Gi
```

### 6.2 äº‘å­˜å‚¨é›†æˆ

**S3 å¤–éƒ¨è¡¨**
```sql
-- åˆ›å»º S3 å¤–éƒ¨è¡¨
CREATE EXTERNAL TABLE s3_orders (
    order_id BIGINT,
    user_id BIGINT,
    amount DECIMAL(10,2),
    order_date DATE
) ENGINE=OLAP
PROPERTIES (
    "type" = "es",
    "hosts" = "http://s3.amazonaws.com/my-bucket/orders/",
    "user" = "access_key",
    "password" = "secret_key",
    "format" = "parquet"
);
```

**å¯¹è±¡å­˜å‚¨å¤‡ä»½**
```sql
-- å¤‡ä»½åˆ° S3
BACKUP SNAPSHOT warehouse.snapshot_20231201
TO `s3://backup-bucket/starrocks-backup/`
PROPERTIES (
    "type" = "s3",
    "s3.endpoint" = "s3.amazonaws.com",
    "s3.region" = "us-west-2",
    "s3.access_key" = "your-access-key",
    "s3.secret_key" = "your-secret-key"
);

-- ä» S3 æ¢å¤
RESTORE SNAPSHOT warehouse.snapshot_20231201
FROM `s3://backup-bucket/starrocks-backup/`
PROPERTIES (
    "backup_timestamp" = "2023-12-01-10-00-00"
);
```

## 7. ç›‘æ§å’Œè¿ç»´é›†æˆ

### 7.1 Prometheus é›†æˆ

**Metrics é‡‡é›†é…ç½®**
```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'starrocks-fe'
    static_configs:
      - targets: ['starrocks-fe:8030']
    metrics_path: '/metrics'
    scrape_interval: 30s
    
  - job_name: 'starrocks-be'
    static_configs:
      - targets: ['starrocks-be1:8040', 'starrocks-be2:8040', 'starrocks-be3:8040']
    metrics_path: '/metrics'
    scrape_interval: 30s
```

**è‡ªå®šä¹‰æŒ‡æ ‡æŸ¥è¯¢**
```sql
-- åˆ›å»ºç›‘æ§æŒ‡æ ‡è¡¨
CREATE TABLE monitoring_metrics (
    metric_time DATETIME,
    metric_name VARCHAR(100),
    metric_value DOUBLE,
    tags JSON
) ENGINE=OLAP
DUPLICATE KEY(metric_time, metric_name)
PARTITION BY RANGE(metric_time) (
    -- åŠ¨æ€åˆ†åŒºé…ç½®
)
DISTRIBUTED BY HASH(metric_name) BUCKETS 16;

-- å®šæœŸé‡‡é›†ç³»ç»ŸæŒ‡æ ‡
INSERT INTO monitoring_metrics
SELECT 
    NOW() as metric_time,
    'query_qps' as metric_name,
    COUNT(*) / 60 as metric_value,  -- QPS
    JSON_OBJECT('database', database_name) as tags
FROM information_schema.query_log
WHERE query_start_time >= NOW() - INTERVAL 1 MINUTE
GROUP BY database_name;
```

### 7.2 Grafana é›†æˆ

**Dashboard é…ç½®**
```json
{
  "dashboard": {
    "title": "StarRocks Monitoring",
    "panels": [
      {
        "title": "Query QPS",
        "type": "graph",
        "targets": [{
          "expr": "rate(starrocks_fe_query_total[5m])",
          "legendFormat": "QPS"
        }]
      },
      {
        "title": "Storage Usage",
        "type": "graph", 
        "targets": [{
          "expr": "starrocks_be_storage_used_bytes",
          "legendFormat": "Used Storage"
        }]
      }
    ]
  }
}
```

## 8. æœ€ä½³å®è·µæ€»ç»“

### 8.1 æ€§èƒ½ä¼˜åŒ–å»ºè®®

**æ•°æ®å¯¼å…¥ä¼˜åŒ–**
- é€‰æ‹©åˆé€‚çš„å¯¼å…¥æ–¹å¼ï¼ˆStream Load vs Routine Loadï¼‰
- åˆç†è®¾ç½®æ‰¹é‡å¤§å°å’Œå¹¶å‘æ•°
- ä½¿ç”¨åˆ—å­˜æ ¼å¼å’Œå‹ç¼©ç®—æ³•
- é¿å…å°æ–‡ä»¶è¿‡å¤šé—®é¢˜

**æŸ¥è¯¢ä¼˜åŒ–**
- åˆ©ç”¨åˆ†åŒºè£å‰ªå’Œåˆ—è£å‰ª
- åˆç†ä½¿ç”¨ç‰©åŒ–è§†å›¾
- ä¼˜åŒ– Join é¡ºåºå’Œæ–¹å¼
- ç›‘æ§æŸ¥è¯¢æ€§èƒ½æŒ‡æ ‡

### 8.2 è¿ç»´ç®¡ç†å»ºè®®

**ç›‘æ§å‘Šè­¦**
- å»ºç«‹å®Œå–„çš„æŒ‡æ ‡ç›‘æ§ä½“ç³»
- è®¾ç½®åˆç†çš„å‘Šè­¦é˜ˆå€¼
- è‡ªåŠ¨åŒ–æ•…éšœå¤„ç†æµç¨‹
- å®šæœŸæ€§èƒ½åŸºå‡†æµ‹è¯•

**å®¹é‡è§„åˆ’**
- é¢„ä¼°æ•°æ®å¢é•¿è¶‹åŠ¿
- åˆç†è§„åˆ’é›†ç¾¤æ‰©å®¹è®¡åˆ’
- ä¼˜åŒ–èµ„æºé…ç½®å’Œæˆæœ¬
- åˆ¶å®šæ•°æ®ç”Ÿå‘½å‘¨æœŸç­–ç•¥

### 8.3 å®‰å…¨è€ƒè™‘

**è®¿é—®æ§åˆ¶**
- é…ç½®ç”¨æˆ·æƒé™å’Œè§’è‰²
- å¯ç”¨ SSL/TLS åŠ å¯†
- ç½‘ç»œå®‰å…¨éš”ç¦»
- å®¡è®¡æ—¥å¿—è®°å½•

**æ•°æ®ä¿æŠ¤**
- å®šæœŸå¤‡ä»½å’Œæ¢å¤æµ‹è¯•
- æ•æ„Ÿæ•°æ®è„±æ•å¤„ç†
- ç¬¦åˆæ•°æ®åˆè§„è¦æ±‚
- å¼‚åœ°å®¹ç¾ç­–ç•¥

StarRocks çš„å¤§æ•°æ®ç”Ÿæ€é›†æˆèƒ½åŠ›ä¸ºæ„å»ºç°ä»£åŒ–æ•°æ®å¹³å°æä¾›äº†å¼ºå¤§æ”¯æŒï¼Œé€šè¿‡åˆç†çš„æ¶æ„è®¾è®¡å’Œä¼˜åŒ–é…ç½®ï¼Œå¯ä»¥æ„å»ºé«˜æ€§èƒ½ã€é«˜å¯é çš„æ•°æ®å¤„ç†å’Œåˆ†æç³»ç»Ÿã€‚

---
## ğŸ“– å¯¼èˆª
[ğŸ  è¿”å›ä¸»é¡µ](../../README.md) | [â¬…ï¸ ä¸Šä¸€é¡µ](cloud-native-architecture.md) | [â¡ï¸ ä¸‹ä¸€é¡µ](../07-best-practices/production-deployment.md)
---