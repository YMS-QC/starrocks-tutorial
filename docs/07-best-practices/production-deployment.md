---
## ğŸ“– å¯¼èˆª
[ğŸ  è¿”å›ä¸»é¡µ](../../README.md) | [â¬…ï¸ ä¸Šä¸€é¡µ](../06-advanced-features/big-data-ecosystem.md) | [â¡ï¸ ä¸‹ä¸€é¡µ](oracle-migration-best-practices.md)
---

# ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å»ºè®®

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½² StarRocks éœ€è¦è€ƒè™‘ç¡¬ä»¶è§„åˆ’ã€é›†ç¾¤è§„æ¨¡ã€å®‰å…¨é…ç½®ã€ç›‘æ§å‘Šè­¦ã€å¤‡ä»½æ¢å¤ç­‰å¤šä¸ªæ–¹é¢ã€‚æœ¬ç« èŠ‚æä¾›å®Œæ•´çš„ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µæŒ‡å—ã€‚

## 1. ç¡¬ä»¶è§„åˆ’å’Œé…ç½®

### 1.1 FE (Frontend) èŠ‚ç‚¹é…ç½®

**æ¨èé…ç½®**

| é›†ç¾¤è§„æ¨¡ | CPU | å†…å­˜ | ç£ç›˜ | ç½‘ç»œ | èŠ‚ç‚¹æ•° |
|---------|-----|------|------|------|-------|
| **å°è§„æ¨¡** (< 10TB) | 8æ ¸+ | 32GB+ | SSD 500GB+ | 1Gbps | 3ä¸ª |
| **ä¸­è§„æ¨¡** (10TB-100TB) | 16æ ¸+ | 64GB+ | SSD 1TB+ | 10Gbps | 3ä¸ª |
| **å¤§è§„æ¨¡** (100TB+) | 32æ ¸+ | 128GB+ | SSD 2TB+ | 10Gbps | 3ä¸ª |

**è¯¦ç»†é…ç½®è¯´æ˜**
```bash
# FE èŠ‚ç‚¹ç¡¬ä»¶æ£€æŸ¥è„šæœ¬
#!/bin/bash
# fe_hardware_check.sh

echo "=== FE èŠ‚ç‚¹ç¡¬ä»¶æ£€æŸ¥ ==="

# CPU æ£€æŸ¥
cpu_cores=$(nproc)
echo "CPU æ ¸å¿ƒæ•°: $cpu_cores"
if [ $cpu_cores -lt 8 ]; then
    echo "âš ï¸  è­¦å‘Š: CPU æ ¸å¿ƒæ•°ä½äºæ¨èå€¼(8æ ¸)"
fi

# å†…å­˜æ£€æŸ¥
memory_gb=$(free -g | grep '^Mem:' | awk '{print $2}')
echo "å†…å­˜å¤§å°: ${memory_gb}GB"
if [ $memory_gb -lt 32 ]; then
    echo "âš ï¸  è­¦å‘Š: å†…å­˜å¤§å°ä½äºæ¨èå€¼(32GB)"
fi

# ç£ç›˜æ£€æŸ¥
disk_size=$(df -h /opt/starrocks | tail -1 | awk '{print $2}')
echo "ç£ç›˜ç©ºé—´: $disk_size"

# ç½‘ç»œæ£€æŸ¥
network_speed=$(ethtool eth0 | grep Speed | awk '{print $2}')
echo "ç½‘ç»œé€Ÿåº¦: $network_speed"

# JVM é…ç½®å»ºè®®
echo ""
echo "=== FE JVM é…ç½®å»ºè®® ==="
echo "JAVA_OPTS=\"-Xmx$((memory_gb * 3 / 4))g -XX:+UseG1GC -XX:G1HeapRegionSize=32m\""
```

**FE é…ç½®æ–‡ä»¶ä¼˜åŒ–**
```bash
# fe.conf ç”Ÿäº§ç¯å¢ƒé…ç½®
priority_networks = 192.168.1.0/24
edit_log_port = 9010
http_port = 8030
rpc_port = 9020
query_port = 9030

# å…ƒæ•°æ®å­˜å‚¨
meta_dir = /opt/starrocks/fe/meta

# æ—¥å¿—é…ç½®
log_roll_size_mb = 1024
sys_log_delete_age = 7d
audit_log_delete_age = 30d

# æŸ¥è¯¢é…ç½®
qe_max_connection = 4096
max_conn_per_user = 1000

# å†…å­˜é…ç½®
query_timeout = 300
max_query_retry_time = 3

# é›†ç¾¤é…ç½®
heartbeat_timeout_second = 30
bdbje_heartbeat_timeout_second = 30
```

### 1.2 BE (Backend) èŠ‚ç‚¹é…ç½®

**æ¨èé…ç½®**

| æ•°æ®ç±»å‹ | CPU | å†…å­˜ | æ•°æ®ç›˜ | æ—¥å¿—ç›˜ | ç½‘ç»œ | å»ºè®®æ•°é‡ |
|----------|-----|------|--------|--------|------|----------|
| **çƒ­æ•°æ®** | 32æ ¸+ | 128GB+ | NVMe SSD 2TB+ Ã— 12 | SSD 500GB | 25Gbps | æŒ‰éœ€æ‰©å±• |
| **æ¸©æ•°æ®** | 16æ ¸+ | 64GB+ | SSD 4TB+ Ã— 6 | SSD 500GB | 10Gbps | æŒ‰éœ€æ‰©å±• |
| **å†·æ•°æ®** | 8æ ¸+ | 32GB+ | SATA 8TB+ Ã— 4 | SSD 500GB | 1Gbps | æŒ‰éœ€æ‰©å±• |

**BE é…ç½®ä¼˜åŒ–**
```bash
# be.conf ç”Ÿäº§ç¯å¢ƒé…ç½®
priority_networks = 192.168.1.0/24
be_port = 9060
webserver_port = 8040
heartbeat_service_port = 9050
brpc_port = 8060

# å­˜å‚¨é…ç½®
storage_root_path = /data1/starrocks,medium:SSD;/data2/starrocks,medium:SSD;/data3/starrocks,medium:HDD

# å†…å­˜é…ç½®
mem_limit = 90%  # ä½¿ç”¨90%ç³»ç»Ÿå†…å­˜
chunk_reserved_bytes_limit = 20%
load_process_max_memory_limit_bytes = 107374182400  # 100GB

# æŸ¥è¯¢é…ç½®
scanner_thread_pool_thread_num = 48
scan_range_max_bytes = 268435456  # 256MB
max_scan_key_num = 1024
max_pushdown_conditions_per_column = 1024

# å‹ç¼©é…ç½®
default_rowset_type = beta
compression_type = LZ4_FRAME

# å¹¶å‘é…ç½®  
pipeline_dop = 0  # 0è¡¨ç¤ºè‡ªåŠ¨
max_fragment_instances_per_be = 64

# ç£ç›˜é…ç½®
disable_storage_page_cache = false
index_stream_cache_capacity = 10737418240  # 10GB
```

### 1.3 å­˜å‚¨è§„åˆ’

**åˆ†å±‚å­˜å‚¨ç­–ç•¥**
```sql
-- é…ç½®å­˜å‚¨ä»‹è´¨
ALTER SYSTEM ADD BACKEND "be1:9050" 
PROPERTIES ("tag.location" = "rack1", "tag.medium" = "SSD");

ALTER SYSTEM ADD BACKEND "be2:9050"
PROPERTIES ("tag.location" = "rack2", "tag.medium" = "HDD");

-- è¡¨çº§åˆ«å­˜å‚¨é…ç½®
CREATE TABLE hot_data (
    id BIGINT,
    data_time DATETIME,
    content STRING
) ENGINE=OLAP
DUPLICATE KEY(id)
PARTITION BY RANGE(data_time) ()
DISTRIBUTED BY HASH(id) BUCKETS 32
PROPERTIES (
    "replication_num" = "3",
    "storage_medium" = "SSD",
    "storage_cooldown_time" = "7d"  -- 7å¤©åè½¬ä¸ºHDD
);
```

**ç£ç›˜ç›‘æ§è„šæœ¬**
```bash
#!/bin/bash
# disk_monitor.sh

check_disk_usage() {
    local path=$1
    local threshold=${2:-85}  # é»˜è®¤85%é˜ˆå€¼
    
    usage=$(df -h "$path" | tail -1 | awk '{print $5}' | sed 's/%//')
    
    if [ "$usage" -gt "$threshold" ]; then
        echo "âš ï¸  è­¦å‘Š: $path ç£ç›˜ä½¿ç”¨ç‡ ${usage}% è¶…è¿‡é˜ˆå€¼ ${threshold}%"
        return 1
    else
        echo "âœ… $path ç£ç›˜ä½¿ç”¨ç‡æ­£å¸¸: ${usage}%"
        return 0
    fi
}

# æ£€æŸ¥æ‰€æœ‰æ•°æ®ç›®å½•
data_dirs="/data1/starrocks /data2/starrocks /data3/starrocks /data4/starrocks"
for dir in $data_dirs; do
    if [ -d "$dir" ]; then
        check_disk_usage "$dir" 85
    fi
done

# æ£€æŸ¥æ—¥å¿—ç›®å½•
check_disk_usage "/opt/starrocks/fe/log" 80
check_disk_usage "/opt/starrocks/be/log" 80
```

## 2. ç½‘ç»œå’Œå®‰å…¨é…ç½®

### 2.1 ç½‘ç»œè§„åˆ’

**ç½‘ç»œæ‹“æ‰‘å»ºè®®**
```
                    åº”ç”¨å±‚
                      |
               Load Balancer
                      |
            +--------+--------+
            |                 |
         FE Cluster      BE Cluster
      (Management VIP)   (Data Network)
            |                 |
    +-------+-------+   +----+----+
    |       |       |   |    |    |
   FE1     FE2     FE3  BE1  BE2  BE3...
```

**ç½‘ç»œé…ç½®è„šæœ¬**
```bash
#!/bin/bash
# network_setup.sh

# è®¾ç½®ç½‘ç»œå‚æ•°ä¼˜åŒ–
echo "net.core.rmem_max = 134217728" >> /etc/sysctl.conf
echo "net.core.wmem_max = 134217728" >> /etc/sysctl.conf  
echo "net.ipv4.tcp_rmem = 4096 65536 134217728" >> /etc/sysctl.conf
echo "net.ipv4.tcp_wmem = 4096 65536 134217728" >> /etc/sysctl.conf
echo "net.core.netdev_max_backlog = 30000" >> /etc/sysctl.conf
echo "net.ipv4.tcp_max_syn_backlog = 30000" >> /etc/sysctl.conf
echo "net.ipv4.tcp_timestamps = 1" >> /etc/sysctl.conf
echo "net.ipv4.tcp_tw_recycle = 1" >> /etc/sysctl.conf

# åº”ç”¨é…ç½®
sysctl -p

# é˜²ç«å¢™é…ç½®
systemctl stop firewalld
systemctl disable firewalld

# æˆ–è€…é…ç½®å¿…è¦ç«¯å£
# firewall-cmd --permanent --add-port=8030/tcp  # FE HTTP
# firewall-cmd --permanent --add-port=9010/tcp  # FE Edit Log  
# firewall-cmd --permanent --add-port=9020/tcp  # FE RPC
# firewall-cmd --permanent --add-port=9030/tcp  # FE Query
# firewall-cmd --permanent --add-port=8040/tcp  # BE HTTP
# firewall-cmd --permanent --add-port=8060/tcp  # BE BRPC
# firewall-cmd --permanent --add-port=9050/tcp  # BE Heartbeat
# firewall-cmd --permanent --add-port=9060/tcp  # BE Thrift
# firewall-cmd --reload
```

### 2.2 å®‰å…¨é…ç½®

**ç”¨æˆ·æƒé™ç®¡ç†**
```sql
-- åˆ›å»ºä¸šåŠ¡ç”¨æˆ·
CREATE USER 'app_user'@'%' IDENTIFIED BY 'strong_password';
CREATE USER 'readonly_user'@'%' IDENTIFIED BY 'readonly_password';
CREATE USER 'etl_user'@'%' IDENTIFIED BY 'etl_password';

-- æƒé™åˆ†é…
-- åº”ç”¨ç”¨æˆ·ï¼šè¯»å†™æƒé™
GRANT SELECT, INSERT, UPDATE, DELETE ON warehouse.* TO 'app_user'@'%';

-- åªè¯»ç”¨æˆ·ï¼šæŸ¥è¯¢æƒé™
GRANT SELECT ON warehouse.* TO 'readonly_user'@'%';

-- ETLç”¨æˆ·ï¼šæ•°æ®å¯¼å…¥æƒé™
GRANT SELECT, INSERT, ALTER, CREATE ON warehouse.* TO 'etl_user'@'%';
GRANT LOAD_PRIV ON *.* TO 'etl_user'@'%';

-- æŸ¥çœ‹ç”¨æˆ·æƒé™
SHOW GRANTS FOR 'app_user'@'%';
```

**SSL/TLS é…ç½®**
```bash
# ç”ŸæˆSSLè¯ä¹¦
openssl genrsa -out server-key.pem 2048
openssl req -new -key server-key.pem -out server-req.pem -subj "/CN=starrocks"
openssl x509 -req -in server-req.pem -signkey server-key.pem -out server-cert.pem -days 365

# FE SSLé…ç½®
echo "enable_ssl = true" >> /opt/starrocks/fe/conf/fe.conf
echo "ssl_certificate = /opt/starrocks/ssl/server-cert.pem" >> /opt/starrocks/fe/conf/fe.conf  
echo "ssl_private_key = /opt/starrocks/ssl/server-key.pem" >> /opt/starrocks/fe/conf/fe.conf

# å®¢æˆ·ç«¯è¿æ¥
mysql -h starrocks-fe -P 9030 -u root --ssl-ca=/path/to/ca.pem --ssl-cert=/path/to/client-cert.pem --ssl-key=/path/to/client-key.pem
```

**å®¡è®¡æ—¥å¿—é…ç½®**
```sql
-- å¯ç”¨å®¡è®¡æ—¥å¿—
SET GLOBAL audit_log_policy = 'ALL';
SET GLOBAL slow_query_log_file = '/opt/starrocks/fe/log/slow_query.log';

-- æŸ¥çœ‹å®¡è®¡æ—¥å¿—
SELECT 
    timestamp,
    client_ip,
    user,
    db,
    state,
    query_time,
    scan_bytes,
    scan_rows,
    stmt
FROM information_schema.audit_log
WHERE timestamp >= DATE_SUB(NOW(), INTERVAL 1 DAY)
  AND query_time > 10  -- æŸ¥è¯¢æ—¶é—´è¶…è¿‡10ç§’
ORDER BY timestamp DESC;
```

## 3. é›†ç¾¤è§„æ¨¡è¯„ä¼°

### 3.1 å®¹é‡è§„åˆ’

**æ•°æ®é‡ä¼°ç®—å…¬å¼**
```
åŸå§‹æ•°æ®å¤§å° = è¡¨è®°å½•æ•° Ã— å¹³å‡è®°å½•å¤§å°
å­˜å‚¨ç©ºé—´éœ€æ±‚ = åŸå§‹æ•°æ®å¤§å° Ã— å‹ç¼©æ¯” Ã— å‰¯æœ¬æ•° Ã— (1 + ç´¢å¼•å¼€é”€)

å…¶ä¸­ï¼š
- å‹ç¼©æ¯”: LZ4 çº¦ 3:1, ZSTD çº¦ 4:1
- å‰¯æœ¬æ•°: é€šå¸¸ä¸º 3
- ç´¢å¼•å¼€é”€: çº¦ 10-20%
```

**å®¹é‡è¯„ä¼°è„šæœ¬**
```python
#!/usr/bin/env python3
# capacity_planning.py

def calculate_storage_requirement(
    row_count, 
    avg_row_size_bytes, 
    compression_ratio=3.0, 
    replication_factor=3, 
    index_overhead=0.15,
    growth_factor=1.5  # 50%å¢é•¿é¢„ç•™
):
    """è®¡ç®—å­˜å‚¨éœ€æ±‚"""
    raw_size_gb = (row_count * avg_row_size_bytes) / (1024**3)
    compressed_size_gb = raw_size_gb / compression_ratio
    replicated_size_gb = compressed_size_gb * replication_factor
    total_size_gb = replicated_size_gb * (1 + index_overhead)
    final_size_gb = total_size_gb * growth_factor
    
    return {
        'raw_size_gb': round(raw_size_gb, 2),
        'compressed_size_gb': round(compressed_size_gb, 2),
        'replicated_size_gb': round(replicated_size_gb, 2),
        'total_size_gb': round(total_size_gb, 2),
        'final_size_gb': round(final_size_gb, 2)
    }

def recommend_cluster_size(total_storage_gb, storage_per_node_gb=20000):
    """æ¨èé›†ç¾¤è§„æ¨¡"""
    min_nodes = max(3, int(total_storage_gb / storage_per_node_gb) + 1)
    
    if total_storage_gb < 100:
        return {
            'be_nodes': 3,
            'fe_nodes': 3, 
            'node_type': 'small',
            'cpu_per_be': 16,
            'memory_per_be_gb': 64
        }
    elif total_storage_gb < 1000:
        return {
            'be_nodes': max(6, min_nodes),
            'fe_nodes': 3,
            'node_type': 'medium', 
            'cpu_per_be': 32,
            'memory_per_be_gb': 128
        }
    else:
        return {
            'be_nodes': max(10, min_nodes),
            'fe_nodes': 3,
            'node_type': 'large',
            'cpu_per_be': 64, 
            'memory_per_be_gb': 256
        }

# ä½¿ç”¨ç¤ºä¾‹
tables = [
    {'name': 'orders', 'rows': 100000000, 'avg_size': 200},
    {'name': 'order_details', 'rows': 500000000, 'avg_size': 150},
    {'name': 'customers', 'rows': 10000000, 'avg_size': 300},
    {'name': 'products', 'rows': 1000000, 'avg_size': 500}
]

total_storage = 0
print("=== è¡¨å­˜å‚¨éœ€æ±‚åˆ†æ ===")
for table in tables:
    storage_req = calculate_storage_requirement(table['rows'], table['avg_size'])
    total_storage += storage_req['final_size_gb']
    print(f"{table['name']:15} {storage_req['final_size_gb']:8.1f} GB")

print(f"\næ€»å­˜å‚¨éœ€æ±‚: {total_storage:.1f} GB")

cluster_rec = recommend_cluster_size(total_storage)
print(f"\n=== é›†ç¾¤è§„æ¨¡æ¨è ===")
print(f"BE èŠ‚ç‚¹æ•°: {cluster_rec['be_nodes']}")
print(f"FE èŠ‚ç‚¹æ•°: {cluster_rec['fe_nodes']}")
print(f"èŠ‚ç‚¹ç±»å‹: {cluster_rec['node_type']}")
print(f"BE CPU: {cluster_rec['cpu_per_be']} æ ¸")
print(f"BE å†…å­˜: {cluster_rec['memory_per_be_gb']} GB")
```

### 3.2 æ€§èƒ½åŸºå‡†æµ‹è¯•

**TPC-H åŸºå‡†æµ‹è¯•è„šæœ¬**
```bash
#!/bin/bash
# tpch_benchmark.sh

SCALE_FACTOR=${1:-10}  # é»˜è®¤10GB
STARROCKS_HOST="starrocks-fe"
STARROCKS_PORT="9030"

echo "æ‰§è¡Œ TPC-H SF$SCALE_FACTOR åŸºå‡†æµ‹è¯•"

# ç”Ÿæˆæµ‹è¯•æ•°æ®
echo "ç”Ÿæˆæµ‹è¯•æ•°æ®..."
cd /opt/tpch-tools
./dbgen -s $SCALE_FACTOR

# åˆ›å»ºè¡¨ç»“æ„
echo "åˆ›å»º TPC-H è¡¨ç»“æ„..."
mysql -h $STARROCKS_HOST -P $STARROCKS_PORT -u root < create_tpch_tables.sql

# å¯¼å…¥æ•°æ®
echo "å¯¼å…¥æµ‹è¯•æ•°æ®..."
for table in customer lineitem nation orders part partsupp region supplier; do
    echo "å¯¼å…¥è¡¨: $table"
    curl --location-trusted -u root: \
        -H "column_separator:|" \
        -H "timeout:3600" \
        -T $table.tbl \
        http://$STARROCKS_HOST:8030/api/tpch/$table/_stream_load
done

# æ‰§è¡ŒæŸ¥è¯¢æµ‹è¯•
echo "æ‰§è¡Œ TPC-H æŸ¥è¯¢æµ‹è¯•..."
results_file="tpch_sf${SCALE_FACTOR}_results_$(date +%Y%m%d_%H%M%S).txt"

for i in {1..22}; do
    echo "æ‰§è¡ŒæŸ¥è¯¢ Q$i" | tee -a $results_file
    start_time=$(date +%s.%N)
    
    timeout 600 mysql -h $STARROCKS_HOST -P $STARROCKS_PORT -u root \
        -e "$(cat queries/q$i.sql)" > /dev/null 2>&1
    
    exit_code=$?
    end_time=$(date +%s.%N)
    duration=$(echo "$end_time - $start_time" | bc)
    
    if [ $exit_code -eq 0 ]; then
        echo "Q$i: ${duration}s" | tee -a $results_file
    elif [ $exit_code -eq 124 ]; then
        echo "Q$i: TIMEOUT (>600s)" | tee -a $results_file
    else
        echo "Q$i: ERROR" | tee -a $results_file
    fi
done

echo "åŸºå‡†æµ‹è¯•å®Œæˆï¼Œç»“æœä¿å­˜åœ¨: $results_file"

# è®¡ç®—æ€»ä½“ç»Ÿè®¡
python3 -c "
import re
import sys

results = []
with open('$results_file', 'r') as f:
    for line in f:
        match = re.search(r'Q(\d+): ([\d.]+)s', line)
        if match:
            results.append(float(match.group(2)))

if results:
    print(f'æˆåŠŸæŸ¥è¯¢æ•°: {len(results)}/22')
    print(f'æ€»æ‰§è¡Œæ—¶é—´: {sum(results):.2f}s')
    print(f'å¹³å‡æŸ¥è¯¢æ—¶é—´: {sum(results)/len(results):.2f}s')
    print(f'æœ€å¿«æŸ¥è¯¢æ—¶é—´: {min(results):.2f}s')
    print(f'æœ€æ…¢æŸ¥è¯¢æ—¶é—´: {max(results):.2f}s')
"
```

## 4. ç›‘æ§å’Œå‘Šè­¦

### 4.1 ç›‘æ§ä½“ç³»æ¶æ„

**ç›‘æ§ç»„ä»¶é€‰æ‹©**
```
Prometheus + Grafana + AlertManager
    â†“
StarRocks Exporter
    â†“  
StarRocks Metrics API (/metrics)
    â†“
FE/BE é›†ç¾¤
```

**Prometheus é…ç½®**
```yaml
# prometheus.yml
global:
  scrape_interval: 30s
  evaluation_interval: 30s

rule_files:
  - "starrocks_rules.yml"

scrape_configs:
  - job_name: 'starrocks-fe'
    static_configs:
      - targets: ['fe1:8030', 'fe2:8030', 'fe3:8030']
    metrics_path: '/metrics'
    scrape_interval: 30s
    
  - job_name: 'starrocks-be'
    static_configs:
      - targets: ['be1:8040', 'be2:8040', 'be3:8040', 'be4:8040']
    metrics_path: '/metrics'
    scrape_interval: 30s

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
```

**å‘Šè­¦è§„åˆ™é…ç½®**
```yaml
# starrocks_rules.yml
groups:
  - name: starrocks_cluster
    rules:
    # FE èŠ‚ç‚¹çŠ¶æ€å‘Šè­¦
    - alert: StarRocksFEDown
      expr: up{job="starrocks-fe"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "StarRocks FE èŠ‚ç‚¹ç¦»çº¿"
        description: "FE èŠ‚ç‚¹ {{ $labels.instance }} å·²ç¦»çº¿è¶…è¿‡1åˆ†é’Ÿ"
    
    # BE èŠ‚ç‚¹çŠ¶æ€å‘Šè­¦
    - alert: StarRocksBEDown
      expr: up{job="starrocks-be"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "StarRocks BE èŠ‚ç‚¹ç¦»çº¿"
        description: "BE èŠ‚ç‚¹ {{ $labels.instance }} å·²ç¦»çº¿è¶…è¿‡1åˆ†é’Ÿ"
    
    # æŸ¥è¯¢å»¶è¿Ÿå‘Šè­¦
    - alert: HighQueryLatency
      expr: starrocks_fe_query_latency_ms{quantile="0.95"} > 10000
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "æŸ¥è¯¢å»¶è¿Ÿè¿‡é«˜"
        description: "95åˆ†ä½æŸ¥è¯¢å»¶è¿Ÿ {{ $value }}ms è¶…è¿‡10ç§’"
    
    # ç£ç›˜ä½¿ç”¨ç‡å‘Šè­¦
    - alert: HighDiskUsage
      expr: starrocks_be_disk_usage_percent > 85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜"
        description: "BEèŠ‚ç‚¹ {{ $labels.instance }} ç£ç›˜ä½¿ç”¨ç‡ {{ $value }}% è¶…è¿‡85%"
    
    # å†…å­˜ä½¿ç”¨ç‡å‘Šè­¦
    - alert: HighMemoryUsage
      expr: starrocks_be_memory_usage_percent > 90
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
        description: "BEèŠ‚ç‚¹ {{ $labels.instance }} å†…å­˜ä½¿ç”¨ç‡ {{ $value }}% è¶…è¿‡90%"
```

### 4.2 Grafana ä»ªè¡¨æ¿

**å…³é”®ç›‘æ§æŒ‡æ ‡**
```json
{
  "dashboard": {
    "title": "StarRocks é›†ç¾¤ç›‘æ§",
    "panels": [
      {
        "title": "é›†ç¾¤çŠ¶æ€",
        "type": "stat",
        "targets": [{
          "expr": "count(up{job=~\"starrocks-.*\"} == 1)",
          "legendFormat": "åœ¨çº¿èŠ‚ç‚¹æ•°"
        }]
      },
      {
        "title": "æŸ¥è¯¢ QPS",
        "type": "graph",
        "targets": [{
          "expr": "rate(starrocks_fe_query_total[5m])",
          "legendFormat": "{{instance}}"
        }]
      },
      {
        "title": "æŸ¥è¯¢å»¶è¿Ÿåˆ†å¸ƒ",
        "type": "graph", 
        "targets": [
          {
            "expr": "starrocks_fe_query_latency_ms{quantile=\"0.50\"}",
            "legendFormat": "P50"
          },
          {
            "expr": "starrocks_fe_query_latency_ms{quantile=\"0.95\"}", 
            "legendFormat": "P95"
          },
          {
            "expr": "starrocks_fe_query_latency_ms{quantile=\"0.99\"}",
            "legendFormat": "P99"
          }
        ]
      },
      {
        "title": "ç£ç›˜ä½¿ç”¨ç‡",
        "type": "graph",
        "targets": [{
          "expr": "starrocks_be_disk_usage_percent",
          "legendFormat": "{{instance}}"
        }]
      },
      {
        "title": "å†…å­˜ä½¿ç”¨ç‡",
        "type": "graph", 
        "targets": [{
          "expr": "starrocks_be_memory_usage_percent",
          "legendFormat": "{{instance}}"
        }]
      }
    ]
  }
}
```

### 4.3 è‡ªå®šä¹‰ç›‘æ§è„šæœ¬

```python
#!/usr/bin/env python3
# starrocks_monitor.py

import requests
import json
import time
import logging
from datetime import datetime

class StarRocksMonitor:
    def __init__(self, fe_hosts, be_hosts):
        self.fe_hosts = fe_hosts
        self.be_hosts = be_hosts
        self.logger = self._setup_logger()
    
    def _setup_logger(self):
        logger = logging.getLogger('starrocks_monitor')
        handler = logging.FileHandler('/var/log/starrocks_monitor.log')
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
        return logger
    
    def check_fe_health(self):
        """æ£€æŸ¥FEèŠ‚ç‚¹å¥åº·çŠ¶æ€"""
        healthy_count = 0
        for host in self.fe_hosts:
            try:
                response = requests.get(f'http://{host}:8030/api/health', timeout=10)
                if response.status_code == 200:
                    healthy_count += 1
                    self.logger.info(f"FE {host} å¥åº·æ£€æŸ¥é€šè¿‡")
                else:
                    self.logger.error(f"FE {host} å¥åº·æ£€æŸ¥å¤±è´¥: HTTP {response.status_code}")
            except Exception as e:
                self.logger.error(f"FE {host} è¿æ¥å¤±è´¥: {str(e)}")
        
        return {
            'total': len(self.fe_hosts),
            'healthy': healthy_count,
            'unhealthy': len(self.fe_hosts) - healthy_count
        }
    
    def check_be_health(self):
        """æ£€æŸ¥BEèŠ‚ç‚¹å¥åº·çŠ¶æ€"""
        healthy_count = 0
        for host in self.be_hosts:
            try:
                response = requests.get(f'http://{host}:8040/api/health', timeout=10)
                if response.status_code == 200:
                    healthy_count += 1
                    self.logger.info(f"BE {host} å¥åº·æ£€æŸ¥é€šè¿‡")
                else:
                    self.logger.error(f"BE {host} å¥åº·æ£€æŸ¥å¤±è´¥: HTTP {response.status_code}")
            except Exception as e:
                self.logger.error(f"BE {host} è¿æ¥å¤±è´¥: {str(e)}")
        
        return {
            'total': len(self.be_hosts),
            'healthy': healthy_count,
            'unhealthy': len(self.be_hosts) - healthy_count
        }
    
    def get_cluster_metrics(self):
        """è·å–é›†ç¾¤æŒ‡æ ‡"""
        try:
            # ä»ä¸»FEè·å–é›†ç¾¤çŠ¶æ€
            fe_host = self.fe_hosts[0]
            response = requests.get(f'http://{fe_host}:8030/metrics', timeout=10)
            
            if response.status_code == 200:
                metrics = self._parse_metrics(response.text)
                return metrics
            else:
                self.logger.error(f"è·å–é›†ç¾¤æŒ‡æ ‡å¤±è´¥: HTTP {response.status_code}")
                return None
                
        except Exception as e:
            self.logger.error(f"è·å–é›†ç¾¤æŒ‡æ ‡å¼‚å¸¸: {str(e)}")
            return None
    
    def _parse_metrics(self, metrics_text):
        """è§£æPrometheusæ ¼å¼æŒ‡æ ‡"""
        metrics = {}
        for line in metrics_text.split('\n'):
            if line.startswith('#') or not line.strip():
                continue
            
            parts = line.split(' ')
            if len(parts) == 2:
                metric_name = parts[0]
                metric_value = parts[1]
                metrics[metric_name] = float(metric_value)
        
        return metrics
    
    def send_alert(self, message, level='warning'):
        """å‘é€å‘Šè­¦"""
        alert_data = {
            'timestamp': datetime.now().isoformat(),
            'level': level,
            'message': message,
            'service': 'starrocks'
        }
        
        # å‘é€åˆ°AlertManageræˆ–å…¶ä»–å‘Šè­¦ç³»ç»Ÿ
        try:
            # ç¤ºä¾‹ï¼šå‘é€åˆ°webhook
            webhook_url = "http://alertmanager:9093/api/v1/alerts"
            requests.post(webhook_url, json=[alert_data], timeout=10)
            self.logger.info(f"å‘Šè­¦å·²å‘é€: {message}")
        except Exception as e:
            self.logger.error(f"å‘Šè­¦å‘é€å¤±è´¥: {str(e)}")
    
    def run_health_check(self):
        """æ‰§è¡Œå¥åº·æ£€æŸ¥"""
        self.logger.info("å¼€å§‹æ‰§è¡Œé›†ç¾¤å¥åº·æ£€æŸ¥")
        
        # FEå¥åº·æ£€æŸ¥
        fe_status = self.check_fe_health()
        if fe_status['unhealthy'] > 0:
            self.send_alert(f"FEèŠ‚ç‚¹å¼‚å¸¸: {fe_status['unhealthy']}/{fe_status['total']} èŠ‚ç‚¹ä¸å¥åº·", 'critical')
        
        # BEå¥åº·æ£€æŸ¥  
        be_status = self.check_be_health()
        if be_status['unhealthy'] > 0:
            self.send_alert(f"BEèŠ‚ç‚¹å¼‚å¸¸: {be_status['unhealthy']}/{be_status['total']} èŠ‚ç‚¹ä¸å¥åº·", 'critical')
        
        # é›†ç¾¤æŒ‡æ ‡æ£€æŸ¥
        metrics = self.get_cluster_metrics()
        if metrics:
            # æ£€æŸ¥æŸ¥è¯¢å»¶è¿Ÿ
            if 'starrocks_fe_query_latency_ms_p95' in metrics:
                p95_latency = metrics['starrocks_fe_query_latency_ms_p95']
                if p95_latency > 10000:  # è¶…è¿‡10ç§’
                    self.send_alert(f"æŸ¥è¯¢å»¶è¿Ÿè¿‡é«˜: P95={p95_latency:.2f}ms", 'warning')
            
            # æ£€æŸ¥é”™è¯¯ç‡
            if 'starrocks_fe_query_err_rate' in metrics:
                error_rate = metrics['starrocks_fe_query_err_rate']
                if error_rate > 0.05:  # è¶…è¿‡5%
                    self.send_alert(f"æŸ¥è¯¢é”™è¯¯ç‡è¿‡é«˜: {error_rate:.2%}", 'warning')
        
        self.logger.info("é›†ç¾¤å¥åº·æ£€æŸ¥å®Œæˆ")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    fe_hosts = ["fe1.company.com", "fe2.company.com", "fe3.company.com"]
    be_hosts = ["be1.company.com", "be2.company.com", "be3.company.com", "be4.company.com"]
    
    monitor = StarRocksMonitor(fe_hosts, be_hosts)
    
    # å•æ¬¡æ£€æŸ¥
    monitor.run_health_check()
    
    # æˆ–è€…å®šæœŸæ£€æŸ¥
    # while True:
    #     monitor.run_health_check()
    #     time.sleep(300)  # 5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
```

## 5. å¤‡ä»½å’Œæ¢å¤ç­–ç•¥

### 5.1 å¤‡ä»½ç­–ç•¥è®¾è®¡

**åˆ†å±‚å¤‡ä»½ç­–ç•¥**
```
å…¨é‡å¤‡ä»½: æ¯å‘¨æ‰§è¡Œä¸€æ¬¡ï¼Œä¿ç•™4å‘¨
å¢é‡å¤‡ä»½: æ¯å¤©æ‰§è¡Œä¸€æ¬¡ï¼Œä¿ç•™7å¤©  
å¿«ç…§å¤‡ä»½: æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡ï¼Œä¿ç•™24å°æ—¶
```

**è‡ªåŠ¨åŒ–å¤‡ä»½è„šæœ¬**
```bash
#!/bin/bash
# backup_manager.sh

BACKUP_BASE_DIR="/backup/starrocks"
BACKUP_RETENTION_DAYS=30
MYSQL_HOST="starrocks-fe"
MYSQL_PORT="9030"

# å…¨é‡å¤‡ä»½
full_backup() {
    local backup_name="full_$(date +%Y%m%d_%H%M%S)"
    local backup_path="$BACKUP_BASE_DIR/$backup_name"
    
    echo "å¼€å§‹å…¨é‡å¤‡ä»½: $backup_name"
    
    mysql -h $MYSQL_HOST -P $MYSQL_PORT -u root -e "
        BACKUP SNAPSHOT warehouse.${backup_name}
        TO '${backup_path}'
        PROPERTIES (
            'type' = 'full',
            'timeout' = '3600'
        )
    "
    
    if [ $? -eq 0 ]; then
        echo "âœ… å…¨é‡å¤‡ä»½æˆåŠŸ: $backup_name"
        
        # éªŒè¯å¤‡ä»½
        if verify_backup "$backup_path"; then
            echo "âœ… å¤‡ä»½éªŒè¯é€šè¿‡"
        else
            echo "âŒ å¤‡ä»½éªŒè¯å¤±è´¥"
            return 1
        fi
        
        # æ¸…ç†æ—§å¤‡ä»½
        cleanup_old_backups "full"
        
    else
        echo "âŒ å…¨é‡å¤‡ä»½å¤±è´¥"
        return 1
    fi
}

# å¢é‡å¤‡ä»½
incremental_backup() {
    local backup_name="incr_$(date +%Y%m%d_%H%M%S)"
    local backup_path="$BACKUP_BASE_DIR/$backup_name"
    
    # è·å–æœ€æ–°çš„å…¨é‡å¤‡ä»½
    local base_backup=$(find $BACKUP_BASE_DIR -name "full_*" -type d | sort -r | head -1)
    if [ -z "$base_backup" ]; then
        echo "âŒ æœªæ‰¾åˆ°åŸºç¡€å…¨é‡å¤‡ä»½ï¼Œæ‰§è¡Œå…¨é‡å¤‡ä»½"
        full_backup
        return $?
    fi
    
    local base_backup_name=$(basename "$base_backup")
    echo "å¼€å§‹å¢é‡å¤‡ä»½: $backup_name (åŸºäº $base_backup_name)"
    
    mysql -h $MYSQL_HOST -P $MYSQL_PORT -u root -e "
        BACKUP SNAPSHOT warehouse.${backup_name}
        TO '${backup_path}' 
        PROPERTIES (
            'type' = 'incremental',
            'base_snapshot' = '${base_backup_name}',
            'timeout' = '1800'
        )
    "
    
    if [ $? -eq 0 ]; then
        echo "âœ… å¢é‡å¤‡ä»½æˆåŠŸ: $backup_name"
        cleanup_old_backups "incr"
    else
        echo "âŒ å¢é‡å¤‡ä»½å¤±è´¥"
        return 1
    fi
}

# å¤‡ä»½éªŒè¯
verify_backup() {
    local backup_path=$1
    
    # æ£€æŸ¥å¤‡ä»½æ–‡ä»¶å®Œæ•´æ€§
    if [ ! -f "$backup_path/meta" ]; then
        echo "âŒ å¤‡ä»½å…ƒæ•°æ®æ–‡ä»¶ç¼ºå¤±"
        return 1
    fi
    
    # æ£€æŸ¥å¤‡ä»½å¤§å°
    local backup_size=$(du -s "$backup_path" | awk '{print $1}')
    if [ $backup_size -lt 1000 ]; then  # å°äº1MBè®¤ä¸ºå¼‚å¸¸
        echo "âŒ å¤‡ä»½æ–‡ä»¶å¼‚å¸¸å°: ${backup_size}KB"
        return 1
    fi
    
    echo "âœ… å¤‡ä»½éªŒè¯é€šè¿‡: ${backup_size}KB"
    return 0
}

# æ¸…ç†æ—§å¤‡ä»½
cleanup_old_backups() {
    local backup_type=$1
    local pattern="${backup_type}_*"
    
    echo "æ¸…ç†æ—§å¤‡ä»½: $pattern"
    find $BACKUP_BASE_DIR -name "$pattern" -type d -mtime +$BACKUP_RETENTION_DAYS -exec rm -rf {} \;
}

# æ¢å¤åŠŸèƒ½
restore_backup() {
    local backup_name=$1
    local restore_database=${2:-warehouse_restored}
    
    if [ -z "$backup_name" ]; then
        echo "ç”¨æ³•: restore_backup <backup_name> [target_database]"
        return 1
    fi
    
    local backup_path="$BACKUP_BASE_DIR/$backup_name"
    if [ ! -d "$backup_path" ]; then
        echo "âŒ å¤‡ä»½ä¸å­˜åœ¨: $backup_path"
        return 1
    fi
    
    echo "å¼€å§‹æ¢å¤å¤‡ä»½: $backup_name -> $restore_database"
    
    mysql -h $MYSQL_HOST -P $MYSQL_PORT -u root -e "
        RESTORE SNAPSHOT ${restore_database}.${backup_name}
        FROM '${backup_path}'
        PROPERTIES (
            'backup_timestamp' = '$(date +%Y-%m-%d-%H-%M-%S)',
            'timeout' = '3600'
        )
    "
    
    if [ $? -eq 0 ]; then
        echo "âœ… å¤‡ä»½æ¢å¤æˆåŠŸ"
    else
        echo "âŒ å¤‡ä»½æ¢å¤å¤±è´¥"
        return 1
    fi
}

# ä¸»å‡½æ•°
main() {
    case "$1" in
        "full")
            full_backup
            ;;
        "incremental"|"incr")
            incremental_backup
            ;;
        "restore")
            restore_backup "$2" "$3"
            ;;
        "cleanup")
            cleanup_old_backups "full"
            cleanup_old_backups "incr"
            ;;
        *)
            echo "ç”¨æ³•: $0 {full|incremental|restore|cleanup}"
            echo "æ¢å¤ç”¨æ³•: $0 restore <backup_name> [target_database]"
            exit 1
            ;;
    esac
}

main "$@"
```

**å®šæ—¶ä»»åŠ¡é…ç½®**
```bash
# crontab -e
# æ¯å‘¨æ—¥ 2:00 æ‰§è¡Œå…¨é‡å¤‡ä»½
0 2 * * 0 /opt/scripts/backup_manager.sh full >> /var/log/backup.log 2>&1

# æ¯å¤© 3:00 æ‰§è¡Œå¢é‡å¤‡ä»½ï¼ˆé™¤å‘¨æ—¥ï¼‰
0 3 * * 1-6 /opt/scripts/backup_manager.sh incr >> /var/log/backup.log 2>&1

# æ¯æœˆ1å· 4:00 æ¸…ç†æ—§å¤‡ä»½
0 4 1 * * /opt/scripts/backup_manager.sh cleanup >> /var/log/backup.log 2>&1
```

### 5.2 ç¾å¤‡æ–¹æ¡ˆ

**è·¨æœºæˆ¿ç¾å¤‡æ¶æ„**
```
ä¸»æœºæˆ¿ (ä¸»é›†ç¾¤)
    â†“ å¼‚æ­¥å¤åˆ¶
å¤‡æœºæˆ¿ (å¤‡é›†ç¾¤)
    â†“ å†·å¤‡ä»½  
å¯¹è±¡å­˜å‚¨ (äº‘å¤‡ä»½)
```

**è·¨æœºæˆ¿åŒæ­¥è„šæœ¬**
```bash
#!/bin/bash  
# disaster_recovery.sh

PRIMARY_CLUSTER="primary-starrocks"
BACKUP_CLUSTER="backup-starrocks"
SYNC_INTERVAL=3600  # 1å°æ—¶åŒæ­¥ä¸€æ¬¡

sync_to_backup_cluster() {
    echo "å¼€å§‹åŒæ­¥æ•°æ®åˆ°å¤‡é›†ç¾¤"
    
    # 1. åœ¨ä¸»é›†ç¾¤åˆ›å»ºå¿«ç…§
    local snapshot_name="dr_sync_$(date +%Y%m%d_%H%M%S)"
    
    mysql -h $PRIMARY_CLUSTER -P 9030 -u root -e "
        BACKUP SNAPSHOT warehouse.${snapshot_name}
        TO 's3://disaster-recovery-bucket/${snapshot_name}'
        PROPERTIES (
            'type' = 'full',
            'timeout' = '7200'
        )
    "
    
    # 2. åœ¨å¤‡é›†ç¾¤æ¢å¤å¿«ç…§
    sleep 300  # ç­‰å¾…å¤‡ä»½å®Œæˆä¼ è¾“
    
    mysql -h $BACKUP_CLUSTER -P 9030 -u root -e "
        RESTORE SNAPSHOT warehouse_backup.${snapshot_name}
        FROM 's3://disaster-recovery-bucket/${snapshot_name}'
        PROPERTIES (
            'timeout' = '7200'
        )
    "
    
    echo "æ•°æ®åŒæ­¥å®Œæˆ: $snapshot_name"
}

# æ•…éšœåˆ‡æ¢
failover_to_backup() {
    echo "å¼€å§‹æ•…éšœåˆ‡æ¢åˆ°å¤‡é›†ç¾¤"
    
    # 1. åœæ­¢ä¸»é›†ç¾¤è®¿é—®
    # 2. æ¿€æ´»å¤‡é›†ç¾¤
    # 3. æ›´æ–°DNSæˆ–è´Ÿè½½å‡è¡¡é…ç½®
    # 4. é€šçŸ¥ä¸šåŠ¡æ–¹åˆ‡æ¢è¿æ¥
    
    echo "æ•…éšœåˆ‡æ¢å®Œæˆ"
}

# ä¸»é›†ç¾¤æ¢å¤
restore_primary() {
    echo "å¼€å§‹æ¢å¤ä¸»é›†ç¾¤"
    
    # 1. ä»å¤‡é›†ç¾¤åŒæ­¥æœ€æ–°æ•°æ®åˆ°ä¸»é›†ç¾¤
    # 2. éªŒè¯æ•°æ®ä¸€è‡´æ€§
    # 3. åˆ‡æ¢å›ä¸»é›†ç¾¤
    # 4. æ¢å¤æ­£å¸¸åŒæ­¥
    
    echo "ä¸»é›†ç¾¤æ¢å¤å®Œæˆ"
}

case "$1" in
    "sync")
        sync_to_backup_cluster
        ;;
    "failover")
        failover_to_backup
        ;;  
    "restore")
        restore_primary
        ;;
    *)
        echo "ç”¨æ³•: $0 {sync|failover|restore}"
        exit 1
        ;;
esac
```

## 6. å‡çº§å’Œç»´æŠ¤

### 6.1 ç‰ˆæœ¬å‡çº§ç­–ç•¥

**æ»šåŠ¨å‡çº§æµç¨‹**
```bash
#!/bin/bash
# rolling_upgrade.sh

OLD_VERSION="2.5.4"
NEW_VERSION="3.0.1"
CLUSTER_NODES="fe1 fe2 fe3 be1 be2 be3 be4"

# å‡çº§å‰æ£€æŸ¥
pre_upgrade_check() {
    echo "æ‰§è¡Œå‡çº§å‰æ£€æŸ¥..."
    
    # æ£€æŸ¥é›†ç¾¤å¥åº·çŠ¶æ€
    for node in $CLUSTER_NODES; do
        if ! ping -c 1 $node > /dev/null 2>&1; then
            echo "âŒ èŠ‚ç‚¹ $node ä¸å¯è¾¾"
            return 1
        fi
    done
    
    # æ£€æŸ¥æ­£åœ¨è¿è¡Œçš„æŸ¥è¯¢
    active_queries=$(mysql -h fe1 -P 9030 -u root -se "
        SELECT COUNT(*) FROM information_schema.processlist 
        WHERE command != 'Sleep'
    ")
    
    if [ $active_queries -gt 10 ]; then
        echo "âš ï¸  è­¦å‘Š: å½“å‰æœ‰ $active_queries ä¸ªæ´»è·ƒæŸ¥è¯¢"
        read -p "æ˜¯å¦ç»§ç»­å‡çº§? (y/N) " -n 1 -r
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            return 1
        fi
    fi
    
    # åˆ›å»ºå‡çº§å‰å¤‡ä»½
    echo "åˆ›å»ºå‡çº§å‰å¤‡ä»½..."
    /opt/scripts/backup_manager.sh full
    
    echo "âœ… å‡çº§å‰æ£€æŸ¥é€šè¿‡"
    return 0
}

# å‡çº§å•ä¸ªèŠ‚ç‚¹
upgrade_node() {
    local node=$1
    local node_type=$2  # fe æˆ– be
    
    echo "å‡çº§èŠ‚ç‚¹: $node ($node_type)"
    
    # 1. åœæ­¢æœåŠ¡
    ssh $node "systemctl stop starrocks-$node_type"
    
    # 2. å¤‡ä»½é…ç½®
    ssh $node "cp -r /opt/starrocks/$node_type/conf /opt/starrocks/${node_type}_conf_backup_$(date +%Y%m%d)"
    
    # 3. å‡çº§è½¯ä»¶åŒ…
    scp starrocks-$NEW_VERSION-$node_type.tar.gz $node:/tmp/
    ssh $node "
        cd /opt/starrocks
        tar -xzf /tmp/starrocks-$NEW_VERSION-$node_type.tar.gz
        mv $node_type ${node_type}_$OLD_VERSION
        mv starrocks-$NEW_VERSION-$node_type $node_type
    "
    
    # 4. æ¢å¤é…ç½®
    ssh $node "cp -r ${node_type}_conf_backup_$(date +%Y%m%d)/* /opt/starrocks/$node_type/conf/"
    
    # 5. å¯åŠ¨æœåŠ¡
    ssh $node "systemctl start starrocks-$node_type"
    
    # 6. å¥åº·æ£€æŸ¥
    sleep 30
    if ssh $node "systemctl is-active starrocks-$node_type" | grep -q "active"; then
        echo "âœ… èŠ‚ç‚¹ $node å‡çº§æˆåŠŸ"
        return 0
    else
        echo "âŒ èŠ‚ç‚¹ $node å‡çº§å¤±è´¥"
        # å›æ»š
        ssh $node "
            systemctl stop starrocks-$node_type
            mv /opt/starrocks/$node_type /opt/starrocks/${node_type}_failed_$NEW_VERSION
            mv /opt/starrocks/${node_type}_$OLD_VERSION /opt/starrocks/$node_type
            systemctl start starrocks-$node_type
        "
        return 1
    fi
}

# æ‰§è¡Œæ»šåŠ¨å‡çº§
rolling_upgrade() {
    echo "å¼€å§‹æ»šåŠ¨å‡çº§: $OLD_VERSION -> $NEW_VERSION"
    
    # 1. å‡çº§å‰æ£€æŸ¥
    if ! pre_upgrade_check; then
        echo "âŒ å‡çº§å‰æ£€æŸ¥å¤±è´¥ï¼Œå–æ¶ˆå‡çº§"
        return 1
    fi
    
    # 2. å‡çº§BEèŠ‚ç‚¹ï¼ˆé€ä¸ªå‡çº§ï¼‰
    for be in be1 be2 be3 be4; do
        if ! upgrade_node $be "be"; then
            echo "âŒ BEèŠ‚ç‚¹ $be å‡çº§å¤±è´¥ï¼Œåœæ­¢å‡çº§"
            return 1
        fi
        
        # ç­‰å¾…é›†ç¾¤ç¨³å®š
        sleep 60
        
        # æ£€æŸ¥é›†ç¾¤çŠ¶æ€
        if ! check_cluster_health; then
            echo "âŒ é›†ç¾¤å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œåœæ­¢å‡çº§"
            return 1
        fi
    done
    
    # 3. å‡çº§FEèŠ‚ç‚¹ï¼ˆé€ä¸ªå‡çº§ï¼Œå…ˆå‡çº§followerï¼‰
    # ç¡®å®šmasterèŠ‚ç‚¹
    master_fe=$(mysql -h fe1 -P 9030 -u root -se "SHOW PROC '/frontends'" | grep "true" | awk '{print $2}')
    
    for fe in fe1 fe2 fe3; do
        if [ "$fe" != "$master_fe" ]; then
            if ! upgrade_node $fe "fe"; then
                echo "âŒ FEèŠ‚ç‚¹ $fe å‡çº§å¤±è´¥ï¼Œåœæ­¢å‡çº§"
                return 1
            fi
            sleep 60
        fi
    done
    
    # 4. æœ€åå‡çº§master FEèŠ‚ç‚¹
    if ! upgrade_node $master_fe "fe"; then
        echo "âŒ Master FEèŠ‚ç‚¹ $master_fe å‡çº§å¤±è´¥"
        return 1
    fi
    
    echo "âœ… æ»šåŠ¨å‡çº§å®Œæˆ"
    return 0
}

# æ£€æŸ¥é›†ç¾¤å¥åº·çŠ¶æ€
check_cluster_health() {
    local fe_count=$(mysql -h fe1 -P 9030 -u root -se "SHOW PROC '/frontends'" | grep -c "true\|false")
    local be_count=$(mysql -h fe1 -P 9030 -u root -se "SHOW PROC '/backends'" | grep -c "true")
    
    if [ $fe_count -ge 3 ] && [ $be_count -ge 4 ]; then
        echo "âœ… é›†ç¾¤å¥åº·æ£€æŸ¥é€šè¿‡: FE=$fe_count, BE=$be_count"
        return 0
    else
        echo "âŒ é›†ç¾¤å¥åº·æ£€æŸ¥å¤±è´¥: FE=$fe_count, BE=$be_count"
        return 1
    fi
}

# ä¸»å‡½æ•°
case "$1" in
    "check")
        pre_upgrade_check
        ;;
    "upgrade")
        rolling_upgrade
        ;;
    "health")
        check_cluster_health
        ;;
    *)
        echo "ç”¨æ³•: $0 {check|upgrade|health}"
        exit 1
        ;;
esac
```

### 6.2 æ—¥å¸¸ç»´æŠ¤ä»»åŠ¡

**ç»´æŠ¤ä»»åŠ¡æ¸…å•**
```bash
#!/bin/bash
# maintenance_tasks.sh

# æ—¥å¿—æ¸…ç†
cleanup_logs() {
    echo "æ¸…ç†è¿‡æœŸæ—¥å¿—æ–‡ä»¶..."
    
    # FEæ—¥å¿—æ¸…ç†
    find /opt/starrocks/fe/log -name "*.log.*" -mtime +7 -delete
    find /opt/starrocks/fe/log -name "*.out.*" -mtime +7 -delete
    
    # BEæ—¥å¿—æ¸…ç†  
    find /opt/starrocks/be/log -name "*.log.*" -mtime +7 -delete
    find /opt/starrocks/be/log -name "*.out.*" -mtime +7 -delete
    
    echo "âœ… æ—¥å¿—æ¸…ç†å®Œæˆ"
}

# ç»Ÿè®¡ä¿¡æ¯æ›´æ–°
update_statistics() {
    echo "æ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯..."
    
    # è·å–æ‰€æœ‰è¡¨åˆ—è¡¨
    tables=$(mysql -h fe1 -P 9030 -u root -se "
        SELECT CONCAT(table_schema, '.', table_name) 
        FROM information_schema.tables 
        WHERE table_schema NOT IN ('information_schema', '_statistics_')
    ")
    
    for table in $tables; do
        echo "æ›´æ–°ç»Ÿè®¡ä¿¡æ¯: $table"
        mysql -h fe1 -P 9030 -u root -e "ANALYZE TABLE $table"
    done
    
    echo "âœ… ç»Ÿè®¡ä¿¡æ¯æ›´æ–°å®Œæˆ"
}

# å‹ç¼©ä¼˜åŒ–
optimize_compaction() {
    echo "è§¦å‘å‹ç¼©ä¼˜åŒ–..."
    
    # è·å–BEåˆ—è¡¨
    be_hosts=$(mysql -h fe1 -P 9030 -u root -se "
        SELECT Host FROM information_schema.be_tablets 
        GROUP BY Host
    ")
    
    for be_host in $be_hosts; do
        echo "è§¦å‘BEå‹ç¼©: $be_host"
        curl -X POST "http://$be_host:8040/api/compaction/run?tablet_id=-1&compact_type=cumulative"
    done
    
    echo "âœ… å‹ç¼©ä¼˜åŒ–å®Œæˆ"  
}

# å­¤ç«‹æ–‡ä»¶æ¸…ç†
cleanup_orphan_files() {
    echo "æ¸…ç†å­¤ç«‹æ–‡ä»¶..."
    
    # é€šè¿‡APIè§¦å‘åƒåœ¾æ¸…ç†
    be_hosts=$(mysql -h fe1 -P 9030 -u root -se "
        SELECT Host FROM information_schema.be_tablets 
        GROUP BY Host  
    ")
    
    for be_host in $be_hosts; do
        echo "æ¸…ç†å­¤ç«‹æ–‡ä»¶: $be_host"
        curl -X POST "http://$be_host:8040/api/trash/clean"
    done
    
    echo "âœ… å­¤ç«‹æ–‡ä»¶æ¸…ç†å®Œæˆ"
}

# æ€§èƒ½åŸºå‡†æµ‹è¯•
run_performance_test() {
    echo "æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•..."
    
    # æ‰§è¡Œç®€å•æŸ¥è¯¢æµ‹è¯•
    start_time=$(date +%s)
    
    mysql -h fe1 -P 9030 -u root -e "
        SELECT 
            COUNT(*) as total_records,
            COUNT(DISTINCT table_name) as table_count
        FROM information_schema.be_tablets
    " > /dev/null
    
    end_time=$(date +%s)
    duration=$((end_time - start_time))
    
    echo "åŸºå‡†æŸ¥è¯¢è€—æ—¶: ${duration}ç§’"
    
    # è®°å½•æ€§èƒ½æŒ‡æ ‡
    echo "$(date): åŸºå‡†æŸ¥è¯¢è€—æ—¶ ${duration}ç§’" >> /var/log/performance_baseline.log
    
    echo "âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ"
}

# å¥åº·æ£€æŸ¥
health_check() {
    echo "æ‰§è¡Œé›†ç¾¤å¥åº·æ£€æŸ¥..."
    
    # æ£€æŸ¥FEçŠ¶æ€
    fe_count=$(mysql -h fe1 -P 9030 -u root -se "
        SELECT COUNT(*) FROM information_schema.frontends WHERE alive = 'true'
    ")
    
    # æ£€æŸ¥BEçŠ¶æ€
    be_count=$(mysql -h fe1 -P 9030 -u root -se "
        SELECT COUNT(*) FROM information_schema.backends WHERE alive = 'true' 
    ")
    
    # æ£€æŸ¥å‰¯æœ¬å¥åº·çŠ¶æ€
    unhealthy_tablets=$(mysql -h fe1 -P 9030 -u root -se "
        SELECT COUNT(*) FROM information_schema.tablet_health 
        WHERE state != 'NORMAL'
    ")
    
    echo "é›†ç¾¤çŠ¶æ€: FE=$fe_count, BE=$be_count, å¼‚å¸¸tablet=$unhealthy_tablets"
    
    if [ $unhealthy_tablets -gt 0 ]; then
        echo "âš ï¸  å‘ç°å¼‚å¸¸tabletï¼Œéœ€è¦å…³æ³¨"
        mysql -h fe1 -P 9030 -u root -e "
            SELECT * FROM information_schema.tablet_health 
            WHERE state != 'NORMAL'
            LIMIT 10
        "
    fi
    
    echo "âœ… å¥åº·æ£€æŸ¥å®Œæˆ"
}

# ä¸»å‡½æ•°
case "$1" in
    "logs")
        cleanup_logs
        ;;
    "stats")
        update_statistics
        ;;
    "compact")
        optimize_compaction
        ;;
    "cleanup")
        cleanup_orphan_files
        ;;
    "performance")
        run_performance_test
        ;;
    "health")
        health_check
        ;;
    "all")
        cleanup_logs
        update_statistics
        optimize_compaction  
        cleanup_orphan_files
        run_performance_test
        health_check
        ;;
    *)
        echo "ç”¨æ³•: $0 {logs|stats|compact|cleanup|performance|health|all}"
        exit 1
        ;;
esac
```

**å®šæ—¶ç»´æŠ¤ä»»åŠ¡**
```bash
# crontab -e

# æ¯å¤©å‡Œæ™¨1ç‚¹æ¸…ç†æ—¥å¿—
0 1 * * * /opt/scripts/maintenance_tasks.sh logs

# æ¯å‘¨æ—¥å‡Œæ™¨2ç‚¹æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
0 2 * * 0 /opt/scripts/maintenance_tasks.sh stats

# æ¯å¤©å‡Œæ™¨3ç‚¹è§¦å‘å‹ç¼©ä¼˜åŒ–
0 3 * * * /opt/scripts/maintenance_tasks.sh compact

# æ¯å‘¨å…­å‡Œæ™¨4ç‚¹æ¸…ç†å­¤ç«‹æ–‡ä»¶
0 4 * * 6 /opt/scripts/maintenance_tasks.sh cleanup

# æ¯å°æ—¶æ‰§è¡Œå¥åº·æ£€æŸ¥
0 * * * * /opt/scripts/maintenance_tasks.sh health

# æ¯å¤©æ—©ä¸Š8ç‚¹æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
0 8 * * * /opt/scripts/maintenance_tasks.sh performance
```

## 7. æœ€ä½³å®è·µæ€»ç»“

### 7.1 éƒ¨ç½²æˆåŠŸå…³é”®å› ç´ 

**ç¡¬ä»¶è§„åˆ’**
- æ ¹æ®æ•°æ®é‡å’ŒæŸ¥è¯¢è´Ÿè½½åˆç†è§„åˆ’ç¡¬ä»¶é…ç½®
- é‡‡ç”¨åˆ†å±‚å­˜å‚¨ç­–ç•¥ä¼˜åŒ–æˆæœ¬
- é¢„ç•™è¶³å¤Ÿçš„æ‰©å±•ç©ºé—´

**ç½‘ç»œè®¾è®¡**
- ä½¿ç”¨é«˜å¸¦å®½ç½‘ç»œï¼Œæ¨èä¸‡å…†ç½‘ç»œ
- åˆç†è§„åˆ’ç½‘ç»œæ‹“æ‰‘ï¼Œé¿å…å•ç‚¹æ•…éšœ
- é…ç½®ç½‘ç»œä¼˜åŒ–å‚æ•°

**å®‰å…¨é…ç½®**
- å®æ–½æœ€å°æƒé™åŸåˆ™
- å¯ç”¨SSL/TLSåŠ å¯†
- é…ç½®å®¡è®¡æ—¥å¿—å’Œç›‘æ§

### 7.2 è¿ç»´ç®¡ç†è¦ç‚¹

**ç›‘æ§å‘Šè­¦**
- å»ºç«‹å…¨é¢çš„ç›‘æ§æŒ‡æ ‡ä½“ç³»
- è®¾ç½®åˆç†çš„å‘Šè­¦é˜ˆå€¼
- å®ç°è‡ªåŠ¨åŒ–æ•…éšœå¤„ç†

**å¤‡ä»½æ¢å¤**
- åˆ¶å®šå®Œå–„çš„å¤‡ä»½ç­–ç•¥
- å®šæœŸéªŒè¯å¤‡ä»½å¯ç”¨æ€§
- å»ºç«‹è·¨æœºæˆ¿ç¾å¤‡æ–¹æ¡ˆ

**å‡çº§ç»´æŠ¤**
- é‡‡ç”¨æ»šåŠ¨å‡çº§ç­–ç•¥
- åšå¥½å‡çº§å‰æµ‹è¯•éªŒè¯
- å»ºç«‹å›æ»šåº”æ€¥é¢„æ¡ˆ

### 7.3 æ€§èƒ½ä¼˜åŒ–å»ºè®®

**è¡¨è®¾è®¡ä¼˜åŒ–**
- åˆç†é€‰æ‹©è¡¨æ¨¡å‹å’Œåˆ†åŒºç­–ç•¥
- ä¼˜åŒ–åˆ†å¸ƒé”®å’Œæ’åºé”®è®¾ç½®
- ä½¿ç”¨ç‰©åŒ–è§†å›¾åŠ é€ŸæŸ¥è¯¢

**é›†ç¾¤è°ƒä¼˜**
- æ ¹æ®ç¡¬ä»¶èµ„æºè°ƒæ•´é…ç½®å‚æ•°
- å®šæœŸåˆ†æå’Œä¼˜åŒ–æ…¢æŸ¥è¯¢
- ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ

ç”Ÿäº§ç¯å¢ƒçš„æˆåŠŸéƒ¨ç½²éœ€è¦åœ¨æŠ€æœ¯ã€æµç¨‹ã€å›¢é˜Ÿç­‰å¤šä¸ªç»´åº¦åšå¥½å……åˆ†å‡†å¤‡ã€‚é€šè¿‡éµå¾ªæœ€ä½³å®è·µï¼Œå¯ä»¥æ„å»ºä¸€ä¸ªé«˜æ€§èƒ½ã€é«˜å¯ç”¨ã€æ˜“ç»´æŠ¤çš„ StarRocks æ•°æ®å¹³å°ã€‚

---
## ğŸ“– å¯¼èˆª
[ğŸ  è¿”å›ä¸»é¡µ](../../README.md) | [â¬…ï¸ ä¸Šä¸€é¡µ](../06-advanced-features/big-data-ecosystem.md) | [â¡ï¸ ä¸‹ä¸€é¡µ](oracle-migration-best-practices.md)
---